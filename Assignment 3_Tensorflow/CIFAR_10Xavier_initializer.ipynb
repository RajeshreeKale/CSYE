{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10Xavier_initializer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Ho7ClD5eIxCY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 3\n",
        "Using Tensorflow to build a CNN network for CIFAR-10 dataset. Each record is of size 1*3072. Building a CNN network to classify the data into the 10 classes.\n",
        "\n",
        "# Dataset\n",
        "CIFAR-10 dataset The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "http://www.cs.utoronto.ca/~kriz/cifar.html\n",
        "\n",
        "# Installing pydrive"
      ]
    },
    {
      "metadata": {
        "id": "Bzgn_Bxisigb",
        "colab_type": "code",
        "outputId": "10f405e3-b7df-4de5-f6fa-f913395510a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\r\u001b[K    1% |▎                               | 10kB 15.9MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 4.9MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 7.0MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 4.4MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 5.4MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 6.4MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 7.2MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 8.0MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 8.8MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 7.1MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 7.3MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 9.5MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 9.5MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 16.9MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 17.1MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 17.0MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 16.7MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 17.1MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 17.1MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 40.7MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 20.9MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 20.7MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 20.4MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 20.2MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 20.3MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 19.7MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 20.5MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 20.6MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 20.5MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 21.9MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 46.6MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 48.1MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 52.4MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 49.0MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 48.5MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 51.2MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 51.1MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 51.3MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 30.2MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 29.4MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 29.1MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 28.6MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 28.4MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 27.5MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 27.5MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 27.9MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 27.7MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 27.6MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 44.8MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 39.1MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 40.0MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 41.6MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 41.8MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 48.3MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 48.9MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 48.4MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 49.1MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 49.5MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 49.4MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 61.6MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 62.2MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 62.4MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 59.9MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 55.2MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 50.8MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 49.8MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 50.1MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 50.2MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 49.6MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 50.1MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 50.3MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 49.5MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 52.0MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 56.1MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 61.6MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 64.1MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 63.8MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 64.2MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 64.1MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 63.6MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 63.7MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 65.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 65.2MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 51.4MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 48.5MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 48.2MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 47.9MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 46.6MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 47.0MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 46.3MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 46.3MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 46.1MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 45.6MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 55.4MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 58.7MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 58.2MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z8SvpNeRJF75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creates connection"
      ]
    },
    {
      "metadata": {
        "id": "GeIudKErthup",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import tensorflow as tf\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q54jfOBiJZXm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticating and creating the PyDrive client"
      ]
    },
    {
      "metadata": {
        "id": "mfb4a-VGtny2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kidgoDNyJte-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Getting ids of all the files in folder"
      ]
    },
    {
      "metadata": {
        "id": "Lxc6qlkettW5",
        "colab_type": "code",
        "outputId": "966e7025-faa3-42af-9a3c-01cdcab0125e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1DCFFw2O6BFq8Gk0eYu7JT4Qn224BNoCt' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: data_batch_1, id: 11Bo2ULl9_aOQ761ONc2vhepnydriELiT\n",
            "title: data_batch_2, id: 1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I\n",
            "title: test_batch, id: 1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp\n",
            "title: data_batch_3, id: 11ky6i6FSTGWJYOzXquELD4H-GUr49C4f\n",
            "title: data_batch_5, id: 1rmRytfjJWua0cv17DzST6PqoDFY2APa6\n",
            "title: data_batch_4, id: 1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5AhgCYufJ6uD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "metadata": {
        "id": "cSGYxQ2Ct3mb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UqRE9xvxKWiU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "p3HnPDl3t-L8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "39dy0ai6Kpdz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# if file is zipped"
      ]
    },
    {
      "metadata": {
        "id": "Ni38sS1KuEHo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_file = drive.CreateFile({'id': '11Bo2ULl9_aOQ761ONc2vhepnydriELiT'})\n",
        "zip_file.GetContentFile('data_batch_1')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I'})\n",
        "zip_file.GetContentFile('data_batch_2')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '11ky6i6FSTGWJYOzXquELD4H-GUr49C4f'})\n",
        "zip_file.GetContentFile('data_batch_3')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh'})\n",
        "zip_file.GetContentFile('data_batch_4')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1rmRytfjJWua0cv17DzST6PqoDFY2APa6'})\n",
        "zip_file.GetContentFile('data_batch_5')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp'})\n",
        "zip_file.GetContentFile('test_batch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e_E-aKjruHNi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data1 = unpickle(\"data_batch_1\")\n",
        "data2 = unpickle(\"data_batch_2\")\n",
        "data3 = unpickle(\"data_batch_3\")\n",
        "data4 = unpickle(\"data_batch_4\")\n",
        "data5 = unpickle(\"data_batch_5\")\n",
        "#label_data = unpickle('../input/batches.meta')[b'label_names']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j3VJSzaZuQg5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels1 = data1[b'labels']\n",
        "data1 = data1[b'data'] * 1.0\n",
        "labels2 = data2[b'labels']\n",
        "data2 = data2[b'data'] * 1.0\n",
        "labels3 = data3[b'labels']\n",
        "data3 = data3[b'data'] * 1.0\n",
        "labels4 = data4[b'labels']\n",
        "data4 = data4[b'data']  * 1.0\n",
        "labels5 = data5[b'labels']\n",
        "data5 = data5[b'data']  * 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wGbUbiiaK_d4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Combine the remaining four arrays to use as training data"
      ]
    },
    {
      "metadata": {
        "id": "PTbFdM3mugrN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_tr = np.concatenate([data1, data2, data3, data4, data5], axis=0)\n",
        "X_tr = np.dstack((X_tr[:, :1024], X_tr[:, 1024:2048], X_tr[:, 2048:])) / 1.0\n",
        "X_tr = (X_tr - 128) / 255.0\n",
        "X_tr = X_tr.reshape(-1, 32, 32, 3)\n",
        "\n",
        "y_tr = np.concatenate([labels1, labels2, labels3, labels4, labels5], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hcGmrIsoLYXP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setting the number of classes"
      ]
    },
    {
      "metadata": {
        "id": "iIlXvN4Buq_g",
        "colab_type": "code",
        "outputId": "1aa5654f-f7ab-4447-88f3-b95278b8fec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "num_classes = len(np.unique(y_tr))\n",
        "\n",
        "print(\"X_tr\", X_tr.shape)\n",
        "print(\"y_tr\", y_tr.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_tr (50000, 32, 32, 3)\n",
            "y_tr (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qKbSRJRRLv8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing the test data"
      ]
    },
    {
      "metadata": {
        "id": "M2VoYuDHuzxM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data = unpickle(\"test_batch\")\n",
        "\n",
        "X_test = test_data[b'data']\n",
        "X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:])) / 1.0\n",
        "X_test = (X_test - 128) / 255.0\n",
        "X_test = X_test.reshape(-1, 32, 32, 3)\n",
        "y_test = np.asarray(test_data[b'labels'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LE0eV9aMMEo0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spliting into test and validation"
      ]
    },
    {
      "metadata": {
        "id": "yFmz1mL8u6Jm",
        "colab_type": "code",
        "outputId": "0d5db50c-b823-47be-9605-471b32902e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X_te, X_cv, y_te, y_cv = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
        "\n",
        "print(\"X_te\", X_te.shape)\n",
        "print(\"X_cv\", X_cv.shape)\n",
        "print(\"y_te\", y_te.shape)\n",
        "print(\"y_cv\", y_cv.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_te (5000, 32, 32, 3)\n",
            "X_cv (5000, 32, 32, 3)\n",
            "y_te (5000,)\n",
            "y_cv (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z85ZkJ0MctFa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch generator"
      ]
    },
    {
      "metadata": {
        "id": "7GwYRF1mvFYZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(X, y, batch_size, crop=False, distort=True):\n",
        "    # Shuffle X,y\n",
        "    shuffled_idx = np.arange(len(y))\n",
        "    np.random.shuffle(shuffled_idx)\n",
        "    i, h, w, c = X.shape\n",
        "    \n",
        "    # Enumerate indexes by steps of batch_size\n",
        "    for i in range(0, len(y), batch_size):\n",
        "        batch_idx = shuffled_idx[i:i+batch_size]\n",
        "        X_return = X[batch_idx]\n",
        "        \n",
        "        # optional random crop of images\n",
        "        if crop:\n",
        "            woff = (w - 24) // 4\n",
        "            hoff = (h - 24) // 4\n",
        "            startw = np.random.randint(low=woff,high=woff*2)\n",
        "            starth = np.random.randint(low=hoff,high=hoff*2)\n",
        "            X_return = X_return[:,startw:startw+24,starth:starth+24,:]\n",
        "       \n",
        "        # do random flipping of images\n",
        "        coin = np.random.binomial(1, 0.5, size=None)\n",
        "        if coin and distort:\n",
        "            X_return = X_return[...,::-1,:]\n",
        "        \n",
        "        yield X_return, y[batch_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lvpnW5M5dCup",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ]
    },
    {
      "metadata": {
        "id": "kDVish_pvLTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 20                   # how many epochs\n",
        "batch_size = 128\n",
        "steps_per_epoch = X_tr.shape[0] / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R7IIxLgyesPI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the network\n",
        "\n",
        "\n",
        "## MODEL 7.13.4.6.7f\n",
        "\n",
        "Model description:\n",
        "\n",
        "*  7.6 - changed kernel reg rate to 0.01 from 0.1\n",
        "* 7.7 - optimize loss instead of ce 7.8 - remove redundant lambda, replaced scale in regularizer with lambda, changed lambda from 0.01 to 0.001\n",
        "* 7.9 - lambda 0 instead of 3\n",
        "* 7.9.1 - lambda 1 instead of 0\n",
        "* 7.9.2 - use lambda 2 instead of 1\n",
        "* 7.9.4f - use 3x3 pooling instead of 2x2\n",
        "* 7.11.6f - add batch norm after conv 5\n",
        "* 7.11.2f - raise lambda, add dropout after fc2\n",
        "* 7.12.2f - change fully connected dropout to 20%\n",
        "* 7.12.2.2g - change fc dropout to 25%, increase filters in last 2 conv layers to 192 from 128\n",
        "* 7.13.2.2f - change all pool sizes to 2x2 from 3x3\n",
        "7.13.3.6f - use different lambda for conv + fc layers"
      ]
    },
    {
      "metadata": {
        "id": "E6BpJ8D_vY6e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create new graph\n",
        "graph = tf.Graph()\n",
        "# whether to retrain model from scratch or use saved model\n",
        "init = True\n",
        "model_name = \"model_7.13.4.7.7l\"\n",
        "\n",
        "with graph.as_default():\n",
        "    # Placeholders\n",
        "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
        "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
        "    training = tf.placeholder(dtype=tf.bool)\n",
        "    \n",
        "    # create global step for decaying learning rate\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # lambda 6\n",
        "    lamC = 0.000050\n",
        "    lamF = 0.0025000\n",
        "           \n",
        "    # learning rate j\n",
        "    epochs_per_decay = 10\n",
        "    starting_rate = 0.003\n",
        "    decay_factor = 0.9\n",
        "    staircase = True\n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(starting_rate,                 # start at 0.003\n",
        "                                               global_step, \n",
        "                                               steps_per_epoch * epochs_per_decay,       # 100 epochs\n",
        "                                               decay_factor,                   # 0.5 decrease\n",
        "                                               staircase=staircase) \n",
        "    \n",
        "    # Small epsilon value for the BN transform\n",
        "    epsilon = 1e-3\n",
        "    \n",
        "    with tf.name_scope('conv1') as scope:\n",
        "        # Convolutional layer 1 \n",
        "        conv1 = tf.layers.conv2d(\n",
        "            X,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=10),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv1'                 \n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn1 = tf.layers.batch_normalization(\n",
        "            conv1,\n",
        "            axis=-1,\n",
        "            momentum=0.99,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn1'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
        "\n",
        "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=9, training=training)\n",
        "    \n",
        "    with tf.name_scope('conv2') as scope:\n",
        "        # Convolutional layer 2\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            conv1_bn_relu,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=8),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv2'                  # Add name\n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn2 = tf.layers.batch_normalization(\n",
        "            conv2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn2'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
        "    \n",
        "    with tf.name_scope('pool1') as scope:\n",
        "         # Max pooling layer 1\n",
        "        pool1 = tf.layers.max_pooling2d(\n",
        "            conv2_bn_relu,                       # Input\n",
        "            pool_size=(2, 2),            # Pool size: 3x3\n",
        "            strides=(2, 2),              # Stride: 2\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            name='pool1'\n",
        "        )\n",
        "\n",
        "        # dropout at 10%\n",
        "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
        "\n",
        "    with tf.name_scope('conv3') as scope:\n",
        "        # Convolutional layer 3\n",
        "        conv3= tf.layers.conv2d(\n",
        "            pool1,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=7),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv3'                 \n",
        "        )\n",
        "\n",
        "        bn3 = tf.layers.batch_normalization(\n",
        "            conv3,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn3'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=0, training=training)\n",
        "\n",
        "    with tf.name_scope('conv4') as scope:\n",
        "        # Convolutional layer 4\n",
        "        conv4= tf.layers.conv2d(\n",
        "            conv3_bn_relu,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv4'                 \n",
        "        )\n",
        "\n",
        "        bn4 = tf.layers.batch_normalization(\n",
        "            conv4,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn4'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
        "    \n",
        "    # Max pooling layer 2 \n",
        "    pool2 = tf.layers.max_pooling2d(\n",
        "        conv4_bn_relu,                       # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool2'\n",
        "    )\n",
        "\n",
        "    with tf.name_scope('conv5') as scope:\n",
        "        # Convolutional layer 5\n",
        "        conv5= tf.layers.conv2d(\n",
        "            pool2,                       # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv5'                 \n",
        "        )\n",
        "        \n",
        "        \n",
        "        bn5 = tf.layers.batch_normalization(\n",
        "            conv5,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn5'\n",
        "        )\n",
        "        \n",
        "        # activation\n",
        "        conv5_bn_relu = tf.nn.relu(bn5, name='relu5')\n",
        "\n",
        "        # try dropout here\n",
        "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
        "\n",
        "    with tf.name_scope('conv6') as scope:\n",
        "        # Convolutional layer 6\n",
        "        conv6= tf.layers.conv2d(\n",
        "            conv5_bn_relu,               # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv6'                \n",
        "        )\n",
        "\n",
        "        bn6 = tf.layers.batch_normalization(\n",
        "            conv6,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn6'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv6_bn_relu = tf.nn.relu(bn6, name='relu6')\n",
        "    \n",
        "    # Max pooling layer 3\n",
        "    pool3 = tf.layers.max_pooling2d(\n",
        "        conv6_bn_relu,               # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool3'\n",
        "    )\n",
        "    \n",
        "    with tf.name_scope('flatten') as scope:\n",
        "        # Flatten output\n",
        "        flat_output = tf.contrib.layers.flatten(pool3)\n",
        "\n",
        "        # dropout at 10%\n",
        "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
        "    \n",
        "    # Fully connected layer 1\n",
        "    with tf.name_scope('fc1') as scope:\n",
        "        fc1 = tf.layers.dense(\n",
        "            flat_output,                 # input\n",
        "            1024,                        # 1024 hidden units\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc1\"\n",
        "        )\n",
        "        \n",
        "        bn7 = tf.layers.batch_normalization(\n",
        "            fc1,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn7'\n",
        "        )\n",
        "        \n",
        "        fc1_relu = tf.nn.relu(bn7, name='fc1_relu')\n",
        "        \n",
        "        # dropout at 25%\n",
        "        fc1_do = tf.layers.dropout(fc1_relu, rate=0.25, seed=10, training=training)\n",
        "    \n",
        "    # Fully connected layer 2\n",
        "    with tf.name_scope('fc2') as scope:\n",
        "        fc2 = tf.layers.dense(\n",
        "            fc1_do,                        # input\n",
        "            512,                        # 512 hidden units\n",
        "            activation=None,            # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=5),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc2\"\n",
        "        )\n",
        "        \n",
        "        bn8 = tf.layers.batch_normalization(\n",
        "            fc2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn8'\n",
        "        )\n",
        "        \n",
        "        fc2_relu = tf.nn.relu(bn8, name='fc2_relu')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        fc2_do = tf.layers.dropout(fc2_relu, rate=0.25, seed=11, training=training)\n",
        "    \n",
        "    # Output layer\n",
        "    logits = tf.layers.dense(\n",
        "        fc2_do,                         # input\n",
        "        num_classes,                           # One output unit per category\n",
        "        activation=None,             # No activation function\n",
        "        kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True,\n",
        "    seed=6,dtype=tf.dtypes.float32),\n",
        "        \n",
        "    )\n",
        "    \n",
        "    # Kernel weights of the 1st conv. layer\n",
        "    with tf.variable_scope('conv1', reuse=True):\n",
        "        conv_kernels1 = tf.get_variable('kernel')\n",
        "    \n",
        "    # Mean cross-entropy\n",
        "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
        "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
        "    \n",
        "    # Adam optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    \n",
        "    # Minimize cross-entropy\n",
        "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Compute predictions and accuracy\n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    is_correct = tf.equal(y, predictions)\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "    \n",
        "    # add this so that the batch norm gets run\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "    # Create summary hooks\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "    tf.summary.scalar('cross_entropy', mean_ce)\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "    \n",
        "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
        "    merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gES7exaXhetY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CONFIGURE OPTIONS"
      ]
    },
    {
      "metadata": {
        "id": "WBBCPVPlvbYe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = True                   # whether to initialize the model or use a saved version\n",
        "crop = False                  # do random cropping of images?\n",
        "\n",
        "meta_data_every = 5\n",
        "log_to_tensorboard = False\n",
        "print_every = 1                # how often to print metrics\n",
        "checkpoint_every = 1           # how often to save model in epochs\n",
        "use_gpu = True                 # whether or not to use the GPU\n",
        "print_metrics = True          # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
        "\n",
        "# Placeholders for metrics\n",
        "if init:\n",
        "    valid_acc_values = []\n",
        "    valid_cost_values = []\n",
        "    train_acc_values = []\n",
        "    train_cost_values = []\n",
        "    train_lr_values = []\n",
        "    train_loss_values = []\n",
        "    \n",
        "\n",
        "config = tf.ConfigProto()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wz4DhflqhywB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trainig the model"
      ]
    },
    {
      "metadata": {
        "id": "PJ4kk5Bzvkx3",
        "colab_type": "code",
        "outputId": "8142ede6-6e01-4003-df78-5c5963a45896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    if log_to_tensorboard:\n",
        "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
        "    \n",
        "    if not print_metrics:\n",
        "        # create a plot to be updated as model is trained\n",
        "        f, ax = plt.subplots(1,3,figsize=(20,5))\n",
        "    \n",
        "    # create the saver\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # If the model is new initialize variables, else restore the session\n",
        "    if init:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "    else:\n",
        "        saver.restore(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "\n",
        "    # Set seed\n",
        "    np.random.seed(0)\n",
        "    \n",
        "    print(\"Training\", model_name, \"...\")\n",
        "    \n",
        "    # Train several epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Accuracy values (train) after each batch\n",
        "        batch_acc = []\n",
        "        batch_cost = []\n",
        "        batch_loss = []\n",
        "        batch_lr = []\n",
        "        \n",
        "        # only log run metadata once per epoch\n",
        "        write_meta_data = False\n",
        "            \n",
        "        for X_batch, y_batch in get_batches(X_tr, y_tr, batch_size, crop=crop, distort=True):\n",
        "            if write_meta_data and log_to_tensboard:\n",
        "                # create the metadata\n",
        "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                run_metadata = tf.RunMetadata()\n",
        "            \n",
        "                # Run training and evaluate accuracy\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                },\n",
        "                options=run_options,\n",
        "                run_metadata=run_metadata)\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                train_writer.add_run_metadata(run_metadata, 'step %d' % step)\n",
        "                train_writer.add_summary(summary, step)\n",
        "                write_meta_data = False\n",
        "                \n",
        "            else:\n",
        "                # Run training without meta data\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                })\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                if log_to_tensorboard:\n",
        "                    train_writer.add_summary(summary, step)\n",
        "\n",
        "        # save checkpoint every nth epoch\n",
        "        if(epoch % checkpoint_every == 0):\n",
        "            print(\"Saving checkpoint\")\n",
        "            # save the model\n",
        "            save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "            # Now that model is saved set init to false so we reload it\n",
        "            init = False\n",
        "        \n",
        "        # init batch arrays\n",
        "        batch_cv_acc = []\n",
        "        batch_cv_cost = []\n",
        "        batch_cv_loss = []\n",
        "        \n",
        "        # Evaluate validation accuracy with batches so as to not crash the GPU\n",
        "        for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size, crop=crop, distort=False):\n",
        "            summary, valid_acc, valid_cost, valid_loss = sess.run([merged, accuracy, mean_ce, loss], feed_dict={\n",
        "                X: X_batch,\n",
        "                y: y_batch,\n",
        "                training: False\n",
        "            })\n",
        "\n",
        "            batch_cv_acc.append(valid_acc)\n",
        "            batch_cv_cost.append(valid_cost)\n",
        "            batch_cv_loss.append(valid_loss)\n",
        "\n",
        "        # Write average of validation data to summary logs\n",
        "        if log_to_tensorboard:\n",
        "            summary = tf.Summary(value=[tf.Summary.Value(tag=\"accuracy\", simple_value=np.mean(batch_cv_acc)),tf.Summary.Value(tag=\"cross_entropy\", simple_value=np.mean(batch_cv_cost)),])\n",
        "            test_writer.add_summary(summary, step)\n",
        "            step += 1\n",
        "            \n",
        "        # take the mean of the values to add to the metrics\n",
        "        valid_acc_values.append(np.mean(batch_cv_acc))\n",
        "        valid_cost_values.append(np.mean(batch_cv_cost))\n",
        "        train_acc_values.append(np.mean(batch_acc))\n",
        "        train_cost_values.append(np.mean(batch_cost))\n",
        "        train_lr_values.append(np.mean(batch_lr))\n",
        "        train_loss_values.append(np.mean(batch_loss))\n",
        "        \n",
        "        if print_metrics:\n",
        "            # Print progress every nth epoch to keep output to reasonable amount\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))\n",
        "        else:\n",
        "            # update the plot\n",
        "            ax[0].cla()\n",
        "            ax[0].plot(valid_acc_values, color=\"red\", label=\"Validation\")\n",
        "            ax[0].plot(train_acc_values, color=\"blue\", label=\"Training\")\n",
        "            ax[0].set_title('Validation accuracy: {:.4f} (mean last 3)'.format(np.mean(valid_acc_values[-3:])))\n",
        "            \n",
        "            # since we can't zoom in on plots like in tensorboard, scale y axis to give a decent amount of detail\n",
        "            if np.mean(valid_acc_values[-3:]) > 0.85:\n",
        "                ax[0].set_ylim([0.75,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.75:\n",
        "                ax[0].set_ylim([0.65,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.65:\n",
        "                ax[0].set_ylim([0.55,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.55:\n",
        "                ax[0].set_ylim([0.45,1.0])           \n",
        "            \n",
        "            ax[0].set_xlabel('Epoch')\n",
        "            ax[0].set_ylabel('Accuracy')\n",
        "            ax[0].legend()\n",
        "            \n",
        "            ax[1].cla()\n",
        "            ax[1].plot(valid_cost_values, color=\"red\", label=\"Validation\")\n",
        "            ax[1].plot(train_cost_values, color=\"blue\", label=\"Training\")\n",
        "            ax[1].set_title('Validation xentropy: {:.3f} (mean last 3)'.format(np.mean(valid_cost_values[-3:])))\n",
        "            ax[1].set_xlabel('Epoch')\n",
        "            ax[1].set_ylabel('Cross Entropy')\n",
        "            ax[1].legend()\n",
        "            \n",
        "            ax[2].cla()\n",
        "            ax[2].plot(train_lr_values)\n",
        "            ax[2].set_title(\"Learning rate: {:.6f}\".format(np.mean(train_lr_values[-1:])))\n",
        "            ax[2].set_xlabel(\"Epoch\")\n",
        "            ax[2].set_ylabel(\"Learning Rate\")\n",
        "            \n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "            \n",
        "        # Print data every 50th epoch so I can write it down to compare models\n",
        "        if (not print_metrics) and (epoch % 50 == 0) and (epoch > 1):\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))  \n",
        "            \n",
        "    # print results of last epoch\n",
        "    print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "                epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "            ))\n",
        "    \n",
        "    # save the session\n",
        "    save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "    # init the test data array\n",
        "    test_acc_values = []\n",
        "    \n",
        "    # Check on the test data\n",
        "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, crop=crop, distort=False):\n",
        "        test_accuracy = sess.run(accuracy, feed_dict={\n",
        "            X: X_batch,\n",
        "            y: y_batch,\n",
        "            training: False\n",
        "        })\n",
        "        test_acc_values.append(test_accuracy)\n",
        "    \n",
        "    # average test accuracy across batches\n",
        "    test_acc = np.mean(test_acc_values)\n",
        "    \n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "# print results of last epoch\n",
        "print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "      epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model_7.13.4.7.7l ...\n",
            "Saving checkpoint\n",
            "Epoch 00 - step 391 - cv acc: 0.514 - train acc: 0.480 (mean) - cv cost: 1.403 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 01 - step 782 - cv acc: 0.680 - train acc: 0.648 (mean) - cv cost: 0.928 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 02 - step 1173 - cv acc: 0.710 - train acc: 0.710 (mean) - cv cost: 0.902 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 03 - step 1564 - cv acc: 0.747 - train acc: 0.743 (mean) - cv cost: 0.733 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 04 - step 1955 - cv acc: 0.753 - train acc: 0.764 (mean) - cv cost: 0.740 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 05 - step 2346 - cv acc: 0.738 - train acc: 0.784 (mean) - cv cost: 0.802 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 06 - step 2737 - cv acc: 0.763 - train acc: 0.797 (mean) - cv cost: 0.731 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 07 - step 3128 - cv acc: 0.774 - train acc: 0.812 (mean) - cv cost: 0.689 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 08 - step 3519 - cv acc: 0.789 - train acc: 0.819 (mean) - cv cost: 0.624 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 09 - step 3910 - cv acc: 0.819 - train acc: 0.827 (mean) - cv cost: 0.553 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 10 - step 4301 - cv acc: 0.818 - train acc: 0.841 (mean) - cv cost: 0.564 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 11 - step 4692 - cv acc: 0.833 - train acc: 0.848 (mean) - cv cost: 0.510 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 12 - step 5083 - cv acc: 0.838 - train acc: 0.854 (mean) - cv cost: 0.503 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 13 - step 5474 - cv acc: 0.824 - train acc: 0.858 (mean) - cv cost: 0.555 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 14 - step 5865 - cv acc: 0.820 - train acc: 0.861 (mean) - cv cost: 0.532 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 15 - step 6256 - cv acc: 0.850 - train acc: 0.866 (mean) - cv cost: 0.455 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 16 - step 6647 - cv acc: 0.848 - train acc: 0.870 (mean) - cv cost: 0.476 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 17 - step 7038 - cv acc: 0.823 - train acc: 0.871 (mean) - cv cost: 0.551 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 18 - step 7429 - cv acc: 0.849 - train acc: 0.875 (mean) - cv cost: 0.458 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 19 - step 7820 - cv acc: 0.851 - train acc: 0.877 (mean) - cv cost: 0.501 - lr: 0.00243\n",
            "Epoch 20 - cv acc: 0.8510 - train acc: 0.8768 (mean) - cv cost: 0.501\n",
            "Epoch 20 - cv acc: 0.8510 - train acc: 0.8768 (mean) - cv cost: 0.501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UzBIB7HQiONx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Scoring and Evaluating trained model"
      ]
    },
    {
      "metadata": {
        "id": "FvTIe9tovylG",
        "colab_type": "code",
        "outputId": "b26d441a-0617-4b98-b3e2-3528404d73bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## MODEL 7.20.0.11g \n",
        "print(\"Model : \", model_name)\n",
        "\n",
        "print(\"Convolutional network accuracy (test set):\",test_acc, \" Validation Set\", valid_acc_values[-1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model :  model_7.13.4.7.7l\n",
            "Convolutional network accuracy (test set): 0.84121096  Validation Set 0.8509766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BQUKhaDTi2cF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "The CIFAR-10 dataset for image classification model using convolutional neural network gave an accuracy of 84% for 20 epoochs. This was the model in which the parameters used were:\n",
        "\n",
        "* Activation Function: Rectified linear unit (ReLU)\n",
        "* Cost function: Cross-Entropy\n",
        "* No.of Epochs: 20\n",
        "* Gradient estimation: ADAM\n",
        "* Network Architecture:Number of layers: 12\n",
        "* Network initialization: xavier initializer\n",
        "\n",
        "\n",
        "I ran it for 20 epochs and got almost 84% accuracy. \n",
        "After changing the Network initialization, the model accuracy improved as the network initialization which is the xavier initializer which makes sure that the weights are ‘just right’, keeping the signal in a reasonable range of values through many layers. \n",
        "\n",
        "I ran it for 20 epochs and got almost 84.12% accuracy. It surely improved than the previous initializer which was a zero initializer.\n",
        "\n",
        "Based on the observations, network plateau of the network using the xavier network initializer that hits during the learning has no change in the accuracy\n",
        "The learning rate changes during the epoochs and in a way improves the accuracy.\n",
        "\n",
        "# References\n",
        "[1] https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
        "\n",
        "[2] https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer\n",
        "\n",
        "[3] https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c\n",
        "\n",
        "[4] https://www.kaggle.com/skooch/cifar-10-in-tensorflow/notebook\n",
        "\n",
        "[5] https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization"
      ]
    }
  ]
}
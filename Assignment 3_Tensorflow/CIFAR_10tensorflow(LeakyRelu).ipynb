{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_10tensorflow(LeakyRelu).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xrsCQCqvA2WL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 3\n",
        "Using Tensorflow to build a CNN network for CIFAR-10 dataset. Each record is of size 1*3072. Building a CNN network to classify the data into the 10 classes.\n",
        "\n",
        "# Dataset\n",
        "CIFAR-10 dataset The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "http://www.cs.utoronto.ca/~kriz/cifar.html\n",
        "\n",
        "# Installing pydrive"
      ]
    },
    {
      "metadata": {
        "id": "lAYnToSuL0Lr",
        "colab_type": "code",
        "outputId": "c92a6197-8b2f-4538-f0b7-7686d964512b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cmblq697I64A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creates connection"
      ]
    },
    {
      "metadata": {
        "id": "DZZpuWD2MKXz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import tensorflow as tf\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LSmbjx_YJOlg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticating and creating the PyDrive client"
      ]
    },
    {
      "metadata": {
        "id": "7qltlPLhMR5v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hiW1OdgBJhSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting ids of all the files in folder"
      ]
    },
    {
      "metadata": {
        "id": "19BEDk75MWF2",
        "colab_type": "code",
        "outputId": "5a3ae744-97e7-4b4d-9ff0-4da59c4e92a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1DCFFw2O6BFq8Gk0eYu7JT4Qn224BNoCt' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: data_batch_1, id: 11Bo2ULl9_aOQ761ONc2vhepnydriELiT\n",
            "title: data_batch_2, id: 1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I\n",
            "title: test_batch, id: 1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp\n",
            "title: data_batch_3, id: 11ky6i6FSTGWJYOzXquELD4H-GUr49C4f\n",
            "title: data_batch_5, id: 1rmRytfjJWua0cv17DzST6PqoDFY2APa6\n",
            "title: data_batch_4, id: 1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d3IlE3TvJxVH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "metadata": {
        "id": "4Z7ZEIyWMjoy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aTd793plKA6g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "iQijyFRQMoJ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lsKKbbrpKeTa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# if file is zipped"
      ]
    },
    {
      "metadata": {
        "id": "-k2T_0d8Mtx8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_file = drive.CreateFile({'id': '11Bo2ULl9_aOQ761ONc2vhepnydriELiT'})\n",
        "zip_file.GetContentFile('data_batch_1')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I'})\n",
        "zip_file.GetContentFile('data_batch_2')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '11ky6i6FSTGWJYOzXquELD4H-GUr49C4f'})\n",
        "zip_file.GetContentFile('data_batch_3')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh'})\n",
        "zip_file.GetContentFile('data_batch_4')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1rmRytfjJWua0cv17DzST6PqoDFY2APa6'})\n",
        "zip_file.GetContentFile('data_batch_5')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp'})\n",
        "zip_file.GetContentFile('test_batch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JJ-oYY6WMyWQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data1 = unpickle(\"data_batch_1\")\n",
        "data2 = unpickle(\"data_batch_2\")\n",
        "data3 = unpickle(\"data_batch_3\")\n",
        "data4 = unpickle(\"data_batch_4\")\n",
        "data5 = unpickle(\"data_batch_5\")\n",
        "#label_data = unpickle('../input/batches.meta')[b'label_names']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qDlmPiijM2sk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels1 = data1[b'labels']\n",
        "data1 = data1[b'data'] * 1.0\n",
        "labels2 = data2[b'labels']\n",
        "data2 = data2[b'data'] * 1.0\n",
        "labels3 = data3[b'labels']\n",
        "data3 = data3[b'data'] * 1.0\n",
        "labels4 = data4[b'labels']\n",
        "data4 = data4[b'data']  * 1.0\n",
        "labels5 = data5[b'labels']\n",
        "data5 = data5[b'data']  * 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cKFiY5fDKziw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Combine the remaining four arrays to use as training data"
      ]
    },
    {
      "metadata": {
        "id": "MaYGw4uzM708",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_tr = np.concatenate([data1, data2, data3, data4, data5], axis=0)\n",
        "X_tr = np.dstack((X_tr[:, :1024], X_tr[:, 1024:2048], X_tr[:, 2048:])) / 1.0\n",
        "X_tr = (X_tr - 128) / 255.0\n",
        "X_tr = X_tr.reshape(-1, 32, 32, 3)\n",
        "\n",
        "y_tr = np.concatenate([labels1, labels2, labels3, labels4, labels5], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Qy9aPybLFGU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Setting the number of classes"
      ]
    },
    {
      "metadata": {
        "id": "w7rkeDrZTU5t",
        "colab_type": "code",
        "outputId": "b5c211c9-f564-41c7-ff23-297e7176c05e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "num_classes = len(np.unique(y_tr))\n",
        "\n",
        "print(\"X_tr\", X_tr.shape)\n",
        "print(\"y_tr\", y_tr.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_tr (50000, 32, 32, 3)\n",
            "y_tr (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kE7xFecILgyl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing the test data"
      ]
    },
    {
      "metadata": {
        "id": "W7y-SXVETagw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data = unpickle(\"test_batch\")\n",
        "\n",
        "X_test = test_data[b'data']\n",
        "X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:])) / 1.0\n",
        "X_test = (X_test - 128) / 255.0\n",
        "X_test = X_test.reshape(-1, 32, 32, 3)\n",
        "y_test = np.asarray(test_data[b'labels'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dae6xf6lL29c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spliting into test and validation"
      ]
    },
    {
      "metadata": {
        "id": "Iao7uKfwTfRc",
        "colab_type": "code",
        "outputId": "e4d065c9-5902-444c-ff84-0bdabdb98851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X_te, X_cv, y_te, y_cv = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
        "\n",
        "print(\"X_te\", X_te.shape)\n",
        "print(\"X_cv\", X_cv.shape)\n",
        "print(\"y_te\", y_te.shape)\n",
        "print(\"y_cv\", y_cv.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_te (5000, 32, 32, 3)\n",
            "X_cv (5000, 32, 32, 3)\n",
            "y_te (5000,)\n",
            "y_cv (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Rl_5ztbAMLmn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch generator"
      ]
    },
    {
      "metadata": {
        "id": "VHMdU3njTncB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(X, y, batch_size, crop=False, distort=True):\n",
        "    # Shuffle X,y\n",
        "    shuffled_idx = np.arange(len(y))\n",
        "    np.random.shuffle(shuffled_idx)\n",
        "    i, h, w, c = X.shape\n",
        "    \n",
        "    # Enumerate indexes by steps of batch_size\n",
        "    for i in range(0, len(y), batch_size):\n",
        "        batch_idx = shuffled_idx[i:i+batch_size]\n",
        "        X_return = X[batch_idx]\n",
        "        \n",
        "        # optional random crop of images\n",
        "        if crop:\n",
        "            woff = (w - 24) // 4\n",
        "            hoff = (h - 24) // 4\n",
        "            startw = np.random.randint(low=woff,high=woff*2)\n",
        "            starth = np.random.randint(low=hoff,high=hoff*2)\n",
        "            X_return = X_return[:,startw:startw+24,starth:starth+24,:]\n",
        "       \n",
        "        # do random flipping of images\n",
        "        coin = np.random.binomial(1, 0.5, size=None)\n",
        "        if coin and distort:\n",
        "            X_return = X_return[...,::-1,:]\n",
        "        \n",
        "        yield X_return, y[batch_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4gqcGiczc1Cm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ]
    },
    {
      "metadata": {
        "id": "ugdpajLkrGDk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 20                  # how many epochs\n",
        "batch_size = 128\n",
        "steps_per_epoch = X_tr.shape[0] / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ogiOOYRhdI6h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the network\n",
        "\n",
        "\n",
        "## MODEL 7.13.4.6.7f\n",
        "\n",
        "Model description:\n",
        "\n",
        "* 7.6 - changed kernel reg rate to 0.01 from 0.1\n",
        "* 7.7 - optimize loss instead of ce 7.8 - remove redundant lambda, replaced scale in regularizer with lambda, changed lambda from 0.01 to 0.001\n",
        "* 7.9 - lambda 0 instead of 3\n",
        "* 7.9.1 - lambda 1 instead of 0\n",
        "* 7.9.2 - use lambda 2 instead of 1\n",
        "* 7.9.4f - use 3x3 pooling instead of 2x2\n",
        "* 7.11.6f - add batch norm after conv 5\n",
        "* 7.11.2f - raise lambda, add dropout after fc2\n",
        "* 7.12.2f - change fully connected dropout to 20%\n",
        "* 7.12.2.2g - change fc dropout to 25%, increase filters in last 2 conv layers to 192 from 128\n",
        "* 7.13.2.2f - change all pool sizes to 2x2 from 3x3\n",
        "* 7.13.3.6f - use different lambda for conv + fc layers"
      ]
    },
    {
      "metadata": {
        "id": "6yB5Id6zrlxF",
        "colab_type": "code",
        "outputId": "3f4d9019-f425-45c0-ff16-c15be196d33d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "cell_type": "code",
      "source": [
        "# Create new graph\n",
        "graph = tf.Graph()\n",
        "# whether to retrain model from scratch or use saved model\n",
        "init = True\n",
        "model_name = \"model_7.13.4.7.7l\"\n",
        "\n",
        "with graph.as_default():\n",
        "    # Placeholders\n",
        "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
        "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
        "    training = tf.placeholder(dtype=tf.bool)\n",
        "    \n",
        "    # create global step for decaying learning rate\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # lambda 6\n",
        "    lamC = 0.000050\n",
        "    lamF = 0.0025000\n",
        "           \n",
        "    # learning rate j\n",
        "    epochs_per_decay = 10\n",
        "    starting_rate = 0.003\n",
        "    decay_factor = 0.9\n",
        "    staircase = True\n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(starting_rate,                 # start at 0.003\n",
        "                                               global_step, \n",
        "                                               steps_per_epoch * epochs_per_decay,       # 100 epochs\n",
        "                                               decay_factor,                   # 0.5 decrease\n",
        "                                               staircase=staircase) \n",
        "    \n",
        "    # Small epsilon value for the BN transform\n",
        "    epsilon = 1e-3\n",
        "    \n",
        "    with tf.name_scope('conv1') as scope:\n",
        "        # Convolutional layer 1 \n",
        "        conv1 = tf.layers.conv2d(\n",
        "            X,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=10),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv1'                 \n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn1 = tf.layers.batch_normalization(\n",
        "            conv1,\n",
        "            axis=-1,\n",
        "            momentum=0.99,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn1'\n",
        "        )\n",
        "\n",
        "        #apply leakyrelu\n",
        "        conv1_bn_leaky_relu = tf.nn.leaky_relu(bn1, name='leaky_relu1')\n",
        "\n",
        "        conv1_bn_leaky_relu = tf.layers.dropout(conv1_bn_leaky_relu, rate=0.1, seed=9, training=training)\n",
        "    \n",
        "    with tf.name_scope('conv2') as scope:\n",
        "        # Convolutional layer 2\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            conv1_bn_leaky_relu,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=8),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv2'                  # Add name\n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn2 = tf.layers.batch_normalization(\n",
        "            conv2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn2'\n",
        "        )\n",
        "\n",
        "        #apply leaky_relu\n",
        "        conv2_bn_leaky_relu = tf.nn.leaky_relu(bn2, name='leaky_relu2')\n",
        "    \n",
        "    with tf.name_scope('pool1') as scope:\n",
        "         # Max pooling layer 1\n",
        "        pool1 = tf.layers.max_pooling2d(\n",
        "            conv2_bn_leaky_relu,                       # Input\n",
        "            pool_size=(2, 2),            # Pool size: 3x3\n",
        "            strides=(2, 2),              # Stride: 2\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            name='pool1'\n",
        "        )\n",
        "\n",
        "        # dropout at 10%\n",
        "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
        "\n",
        "    with tf.name_scope('conv3') as scope:\n",
        "        # Convolutional layer 3\n",
        "        conv3= tf.layers.conv2d(\n",
        "            pool1,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=7),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv3'                 \n",
        "        )\n",
        "\n",
        "        bn3 = tf.layers.batch_normalization(\n",
        "            conv3,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn3'\n",
        "        )\n",
        "\n",
        "        #apply leaky_relu\n",
        "        conv3_bn_leaky_relu = tf.nn.leaky_relu(bn3, name='leaky_relu3')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        conv3_bn_leaky_relu = tf.layers.dropout(conv3_bn_leaky_relu, rate=0.1, seed=0, training=training)\n",
        "\n",
        "    with tf.name_scope('conv4') as scope:\n",
        "        # Convolutional layer 4\n",
        "        conv4= tf.layers.conv2d(\n",
        "            conv3_bn_leaky_relu,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv4'                 \n",
        "        )\n",
        "\n",
        "        bn4 = tf.layers.batch_normalization(\n",
        "            conv4,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn4'\n",
        "        )\n",
        "\n",
        "        #apply leaky_relu\n",
        "        conv4_bn_leaky_relu = tf.nn.leaky_relu(bn4, name='leaky_relu4')\n",
        "    \n",
        "    # Max pooling layer 2 \n",
        "    pool2 = tf.layers.max_pooling2d(\n",
        "        conv4_bn_leaky_relu,                       # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool2'\n",
        "    )\n",
        "\n",
        "    with tf.name_scope('conv5') as scope:\n",
        "        # Convolutional layer 5\n",
        "        conv5= tf.layers.conv2d(\n",
        "            pool2,                       # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv5'                 \n",
        "        )\n",
        "        \n",
        "        \n",
        "        bn5 = tf.layers.batch_normalization(\n",
        "            conv5,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn5'\n",
        "        )\n",
        "        \n",
        "        # activation\n",
        "        conv5_bn_leaky_relu = tf.nn.leaky_relu(bn5, name='leaky_relu5')\n",
        "\n",
        "        # try dropout here\n",
        "        conv5_bn_leaky_relu = tf.layers.dropout(conv5_bn_leaky_relu, rate=0.1, seed=3, training=training)    \n",
        "\n",
        "    with tf.name_scope('conv6') as scope:\n",
        "        # Convolutional layer 6\n",
        "        conv6= tf.layers.conv2d(\n",
        "            conv5_bn_relu,               # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv6'                \n",
        "        )\n",
        "\n",
        "        bn6 = tf.layers.batch_normalization(\n",
        "            conv6,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn6'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv6_bn_leaky_relu = tf.nn.leaky_relu(bn6, name='leaky_relu6')\n",
        "    \n",
        "    # Max pooling layer 3\n",
        "    pool3 = tf.layers.max_pooling2d(\n",
        "        conv6_bn_leaky_relu,               # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool3'\n",
        "    )\n",
        "    \n",
        "    with tf.name_scope('flatten') as scope:\n",
        "        # Flatten output\n",
        "        flat_output = tf.contrib.layers.flatten(pool3)\n",
        "\n",
        "        # dropout at 10%\n",
        "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
        "    \n",
        "    # Fully connected layer 1\n",
        "    with tf.name_scope('fc1') as scope:\n",
        "        fc1 = tf.layers.dense(\n",
        "            flat_output,                 # input\n",
        "            1024,                        # 1024 hidden units\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc1\"\n",
        "        )\n",
        "        \n",
        "        bn7 = tf.layers.batch_normalization(\n",
        "            fc1,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn7'\n",
        "        )\n",
        "        \n",
        "        fc1_leaky_relu = tf.nn.leaky_relu(bn7, name='fc1_leaky_relu')\n",
        "        \n",
        "        # dropout at 25%\n",
        "        fc1_do = tf.layers.dropout(fc1_leaky_relu, rate=0.25, seed=10, training=training)\n",
        "    \n",
        "    # Fully connected layer 2\n",
        "    with tf.name_scope('fc2') as scope:\n",
        "        fc2 = tf.layers.dense(\n",
        "            fc1_do,                        # input\n",
        "            512,                        # 512 hidden units\n",
        "            activation=None,            # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=5),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc2\"\n",
        "        )\n",
        "        \n",
        "        bn8 = tf.layers.batch_normalization(\n",
        "            fc2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn8'\n",
        "        )\n",
        "        \n",
        "        fc2_leaky_relu = tf.nn.leaky_relu(bn8, name='fc2_leaky_relu')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        fc2_do = tf.layers.dropout(fc2_leaky_relu, rate=0.25, seed=11, training=training)\n",
        "    \n",
        "    # Output layer\n",
        "    logits = tf.layers.dense(\n",
        "        fc2_do,                         # input\n",
        "        num_classes,                           # One output unit per category\n",
        "        activation=None,             # No activation function\n",
        "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
        "        bias_initializer=tf.zeros_initializer(),\n",
        "        name=\"logits\"\n",
        "    )\n",
        "    \n",
        "    # Kernel weights of the 1st conv. layer\n",
        "    with tf.variable_scope('conv1', reuse=True):\n",
        "        conv_kernels1 = tf.get_variable('kernel')\n",
        "    \n",
        "    # Mean cross-entropy\n",
        "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
        "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
        "    \n",
        "    # Adam optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    \n",
        "    # Minimize cross-entropy\n",
        "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Compute predictions and accuracy\n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    is_correct = tf.equal(y, predictions)\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "    \n",
        "    # add this so that the batch norm gets run\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "    # Create summary hooks\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "    tf.summary.scalar('cross_entropy', mean_ce)\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "    \n",
        "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
        "    merged = tf.summary.merge_all()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-16-bffe7c231b95>:45: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-bffe7c231b95>:61: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-bffe7c231b95>:67: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-16-bffe7c231b95>:109: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-bffe7c231b95>:284: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TJhYw80NhHxW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CONFIGURE OPTIONS"
      ]
    },
    {
      "metadata": {
        "id": "W0vwP_IGrvkm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = True                   # whether to initialize the model or use a saved version\n",
        "crop = False                  # do random cropping of images?\n",
        "\n",
        "meta_data_every = 5\n",
        "log_to_tensorboard = False\n",
        "print_every = 1                # how often to print metrics\n",
        "checkpoint_every = 1           # how often to save model in epochs\n",
        "use_gpu = True                 # whether or not to use the GPU\n",
        "print_metrics = True          # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
        "\n",
        "# Placeholders for metrics\n",
        "if init:\n",
        "    valid_acc_values = []\n",
        "    valid_cost_values = []\n",
        "    train_acc_values = []\n",
        "    train_cost_values = []\n",
        "    train_lr_values = []\n",
        "    train_loss_values = []\n",
        "    \n",
        "\n",
        "config = tf.ConfigProto()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1Qwmg0zhlE6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trainig the model"
      ]
    },
    {
      "metadata": {
        "id": "VyliXPpmsBJP",
        "colab_type": "code",
        "outputId": "4ba1c9e3-e229-45c0-d281-9781ce853e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    if log_to_tensorboard:\n",
        "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
        "    \n",
        "    if not print_metrics:\n",
        "        # create a plot to be updated as model is trained\n",
        "        f, ax = plt.subplots(1,3,figsize=(20,5))\n",
        "    \n",
        "    # create the saver\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # If the model is new initialize variables, else restore the session\n",
        "    if init:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "    else:\n",
        "        saver.restore(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "\n",
        "    # Set seed\n",
        "    np.random.seed(0)\n",
        "    \n",
        "    print(\"Training\", model_name, \"...\")\n",
        "    \n",
        "    # Train several epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Accuracy values (train) after each batch\n",
        "        batch_acc = []\n",
        "        batch_cost = []\n",
        "        batch_loss = []\n",
        "        batch_lr = []\n",
        "        \n",
        "        # only log run metadata once per epoch\n",
        "        write_meta_data = False\n",
        "            \n",
        "        for X_batch, y_batch in get_batches(X_tr, y_tr, batch_size, crop=crop, distort=True):\n",
        "            if write_meta_data and log_to_tensboard:\n",
        "                # create the metadata\n",
        "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                run_metadata = tf.RunMetadata()\n",
        "            \n",
        "                # Run training and evaluate accuracy\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                },\n",
        "                options=run_options,\n",
        "                run_metadata=run_metadata)\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                train_writer.add_run_metadata(run_metadata, 'step %d' % step)\n",
        "                train_writer.add_summary(summary, step)\n",
        "                write_meta_data = False\n",
        "                \n",
        "            else:\n",
        "                # Run training without meta data\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                })\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                if log_to_tensorboard:\n",
        "                    train_writer.add_summary(summary, step)\n",
        "\n",
        "        # save checkpoint every nth epoch\n",
        "        if(epoch % checkpoint_every == 0):\n",
        "            print(\"Saving checkpoint\")\n",
        "            # save the model\n",
        "            save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "            # Now that model is saved set init to false so we reload it\n",
        "            init = False\n",
        "        \n",
        "        # init batch arrays\n",
        "        batch_cv_acc = []\n",
        "        batch_cv_cost = []\n",
        "        batch_cv_loss = []\n",
        "        \n",
        "        # Evaluate validation accuracy with batches so as to not crash the GPU\n",
        "        for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size, crop=crop, distort=False):\n",
        "            summary, valid_acc, valid_cost, valid_loss = sess.run([merged, accuracy, mean_ce, loss], feed_dict={\n",
        "                X: X_batch,\n",
        "                y: y_batch,\n",
        "                training: False\n",
        "            })\n",
        "\n",
        "            batch_cv_acc.append(valid_acc)\n",
        "            batch_cv_cost.append(valid_cost)\n",
        "            batch_cv_loss.append(valid_loss)\n",
        "\n",
        "        # Write average of validation data to summary logs\n",
        "        if log_to_tensorboard:\n",
        "            summary = tf.Summary(value=[tf.Summary.Value(tag=\"accuracy\", simple_value=np.mean(batch_cv_acc)),tf.Summary.Value(tag=\"cross_entropy\", simple_value=np.mean(batch_cv_cost)),])\n",
        "            test_writer.add_summary(summary, step)\n",
        "            step += 1\n",
        "            \n",
        "        # take the mean of the values to add to the metrics\n",
        "        valid_acc_values.append(np.mean(batch_cv_acc))\n",
        "        valid_cost_values.append(np.mean(batch_cv_cost))\n",
        "        train_acc_values.append(np.mean(batch_acc))\n",
        "        train_cost_values.append(np.mean(batch_cost))\n",
        "        train_lr_values.append(np.mean(batch_lr))\n",
        "        train_loss_values.append(np.mean(batch_loss))\n",
        "        \n",
        "        if print_metrics:\n",
        "            # Print progress every nth epoch to keep output to reasonable amount\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))\n",
        "        else:\n",
        "            # update the plot\n",
        "            ax[0].cla()\n",
        "            ax[0].plot(valid_acc_values, color=\"red\", label=\"Validation\")\n",
        "            ax[0].plot(train_acc_values, color=\"blue\", label=\"Training\")\n",
        "            ax[0].set_title('Validation accuracy: {:.4f} (mean last 3)'.format(np.mean(valid_acc_values[-3:])))\n",
        "            \n",
        "            # since we can't zoom in on plots like in tensorboard, scale y axis to give a decent amount of detail\n",
        "            if np.mean(valid_acc_values[-3:]) > 0.85:\n",
        "                ax[0].set_ylim([0.75,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.75:\n",
        "                ax[0].set_ylim([0.65,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.65:\n",
        "                ax[0].set_ylim([0.55,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.55:\n",
        "                ax[0].set_ylim([0.45,1.0])           \n",
        "            \n",
        "            ax[0].set_xlabel('Epoch')\n",
        "            ax[0].set_ylabel('Accuracy')\n",
        "            ax[0].legend()\n",
        "            \n",
        "            ax[1].cla()\n",
        "            ax[1].plot(valid_cost_values, color=\"red\", label=\"Validation\")\n",
        "            ax[1].plot(train_cost_values, color=\"blue\", label=\"Training\")\n",
        "            ax[1].set_title('Validation xentropy: {:.3f} (mean last 3)'.format(np.mean(valid_cost_values[-3:])))\n",
        "            ax[1].set_xlabel('Epoch')\n",
        "            ax[1].set_ylabel('Cross Entropy')\n",
        "            ax[1].legend()\n",
        "            \n",
        "            ax[2].cla()\n",
        "            ax[2].plot(train_lr_values)\n",
        "            ax[2].set_title(\"Learning rate: {:.6f}\".format(np.mean(train_lr_values[-1:])))\n",
        "            ax[2].set_xlabel(\"Epoch\")\n",
        "            ax[2].set_ylabel(\"Learning Rate\")\n",
        "            \n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "            \n",
        "        # Print data every 50th epoch so I can write it down to compare models\n",
        "        if (not print_metrics) and (epoch % 50 == 0) and (epoch > 1):\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))  \n",
        "            \n",
        "    # print results of last epoch\n",
        "    print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "                epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "            ))\n",
        "    \n",
        "    # save the session\n",
        "    save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "    # init the test data array\n",
        "    test_acc_values = []\n",
        "    \n",
        "    # Check on the test data\n",
        "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, crop=crop, distort=False):\n",
        "        test_accuracy = sess.run(accuracy, feed_dict={\n",
        "            X: X_batch,\n",
        "            y: y_batch,\n",
        "            training: False\n",
        "        })\n",
        "        test_acc_values.append(test_accuracy)\n",
        "    \n",
        "    # average test accuracy across batches\n",
        "    test_acc = np.mean(test_acc_values)\n",
        "    \n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "# print results of last epoch\n",
        "print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "      epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "    ))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model_7.13.4.7.7l ...\n",
            "Saving checkpoint\n",
            "Epoch 00 - step 391 - cv acc: 0.493 - train acc: 0.507 (mean) - cv cost: 1.463 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 01 - step 782 - cv acc: 0.665 - train acc: 0.646 (mean) - cv cost: 0.990 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 02 - step 1173 - cv acc: 0.688 - train acc: 0.696 (mean) - cv cost: 0.940 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 03 - step 1564 - cv acc: 0.737 - train acc: 0.727 (mean) - cv cost: 0.800 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 04 - step 1955 - cv acc: 0.747 - train acc: 0.745 (mean) - cv cost: 0.750 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 05 - step 2346 - cv acc: 0.739 - train acc: 0.766 (mean) - cv cost: 0.808 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 06 - step 2737 - cv acc: 0.787 - train acc: 0.774 (mean) - cv cost: 0.656 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 07 - step 3128 - cv acc: 0.785 - train acc: 0.788 (mean) - cv cost: 0.652 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 08 - step 3519 - cv acc: 0.798 - train acc: 0.796 (mean) - cv cost: 0.615 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 09 - step 3910 - cv acc: 0.791 - train acc: 0.804 (mean) - cv cost: 0.651 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 10 - step 4301 - cv acc: 0.820 - train acc: 0.817 (mean) - cv cost: 0.563 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 11 - step 4692 - cv acc: 0.776 - train acc: 0.825 (mean) - cv cost: 0.688 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 12 - step 5083 - cv acc: 0.829 - train acc: 0.827 (mean) - cv cost: 0.538 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 13 - step 5474 - cv acc: 0.821 - train acc: 0.837 (mean) - cv cost: 0.531 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 14 - step 5865 - cv acc: 0.833 - train acc: 0.835 (mean) - cv cost: 0.517 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 15 - step 6256 - cv acc: 0.821 - train acc: 0.843 (mean) - cv cost: 0.537 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 16 - step 6647 - cv acc: 0.834 - train acc: 0.845 (mean) - cv cost: 0.507 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 17 - step 7038 - cv acc: 0.815 - train acc: 0.848 (mean) - cv cost: 0.579 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 18 - step 7429 - cv acc: 0.836 - train acc: 0.850 (mean) - cv cost: 0.488 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 19 - step 7820 - cv acc: 0.830 - train acc: 0.855 (mean) - cv cost: 0.530 - lr: 0.00243\n",
            "Epoch 20 - cv acc: 0.8301 - train acc: 0.8549 (mean) - cv cost: 0.530\n",
            "Epoch 20 - cv acc: 0.8301 - train acc: 0.8549 (mean) - cv cost: 0.530\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MpjShyCfh-iT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Scoring and Evaluating trained model"
      ]
    },
    {
      "metadata": {
        "id": "VnpDVvdKGLC_",
        "colab_type": "code",
        "outputId": "9c2d72e4-d247-41be-d812-1c46fad3337d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## MODEL 7.20.0.11g \n",
        "print(\"Model : \", model_name)\n",
        "\n",
        "print(\"Convolutional network accuracy (test set):\",test_acc, \" Validation Set\", valid_acc_values[-1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model :  model_7.13.4.7.7l\n",
            "Convolutional network accuracy (test set): 0.8179687  Validation Set 0.8300781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2sBjuSgYiULp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "The CIFAR-10 dataset for image classification model using convolutional neural network gave an accuracy of 81% for 20 epoochs. This was the model in which the parameters used were:\n",
        "\n",
        "* Activation Function: Leaky rectified linear unit (Leaky ReLU)\n",
        "* Cost function: Cross-Entropy\n",
        "* No.of Epochs: 20\n",
        "* Gradient estimation: ADAM\n",
        "* Network Architecture:Number of layers: 12\n",
        "* Network initialization: zero\n",
        "\n",
        "After changing the activation function the model accuracy slightly reduced as the leaky relu function is a modified version of ReLU, introducing a nonzero gradient for negative input. \n",
        "I ran it for 20 epochs and got almost 81% accuracy. It can surely go much further since it was still undertrained! To improve the model the learning rate should be improved. Also, number of neurons can be more complicated for a better fit.Along, with that all, the parameters mentioned above can also be changed in order to improve the model accuracy.\n",
        "\n",
        "Based on the observations, network plateau of the network using the activation function leaky relu that hits during the learning has no change in the accuracy\n",
        "The learning rate from epoochs 1-6 remains the same which is 0.00300 and it reduces down to 0.00270 and further more remains constant\n",
        "\n",
        "# References\n",
        "[1] https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
        "\n",
        "[2] http://www.cs.utoronto.ca/~kriz/cifar.html\n",
        "\n",
        "[3] https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c\n",
        "\n",
        "[4] https://www.kaggle.com/skooch/cifar-10-in-tensorflow/notebook\n",
        "\n",
        "[5]https://www.tensorflow.org/api_docs/python/tf/nn/leaky_relu\n",
        "\n",
        "[6]https://tensorlayer.readthedocs.io/en/stable/modules/activation.html"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_10tensorflow(Model 2).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-5phG2cVz7R4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  ASSIGNMENT 3\n",
        "Using Tensorflow to build a CNN network for CIFAR-10 dataset. Each record is of size 1*3072. Building a CNN network to classify the data into the 10 classes.\n",
        "\n",
        "#Dataset\n",
        "CIFAR-10 dataset The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "http://www.cs.utoronto.ca/~kriz/cifar.html\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kiOXPH1N0GUo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installing pydrive"
      ]
    },
    {
      "metadata": {
        "id": "Lxfj8nCD6qCW",
        "colab_type": "code",
        "outputId": "aab6450c-7792-4a82-b40b-c9d7e9bce0e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.4)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8EFFvgLF0KAY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creates connection "
      ]
    },
    {
      "metadata": {
        "id": "QJb-Ll4e6_Ff",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import tensorflow as tf\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dry6FSFO0QdM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticating and creating the PyDrive client"
      ]
    },
    {
      "metadata": {
        "id": "vyJk2QDF7Fm7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G39golO00YXT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting ids of all the files in folder"
      ]
    },
    {
      "metadata": {
        "id": "9OosZtRx7L7k",
        "colab_type": "code",
        "outputId": "cf3f6c94-ba7d-44a7-c8d7-a34924496dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1DCFFw2O6BFq8Gk0eYu7JT4Qn224BNoCt' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: data_batch_1, id: 11Bo2ULl9_aOQ761ONc2vhepnydriELiT\n",
            "title: data_batch_2, id: 1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I\n",
            "title: test_batch, id: 1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp\n",
            "title: data_batch_3, id: 11ky6i6FSTGWJYOzXquELD4H-GUr49C4f\n",
            "title: data_batch_5, id: 1rmRytfjJWua0cv17DzST6PqoDFY2APa6\n",
            "title: data_batch_4, id: 1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-Q_5zv3p0iMg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "metadata": {
        "id": "MJcUYpDK7Pyg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0_FGHTsz0nR4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "donmHn_e7Taf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_i195nWy0rHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# if file is zipped"
      ]
    },
    {
      "metadata": {
        "id": "Gh5qWABz7Ylh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_file = drive.CreateFile({'id': '11Bo2ULl9_aOQ761ONc2vhepnydriELiT'})\n",
        "zip_file.GetContentFile('data_batch_1')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I'})\n",
        "zip_file.GetContentFile('data_batch_2')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '11ky6i6FSTGWJYOzXquELD4H-GUr49C4f'})\n",
        "zip_file.GetContentFile('data_batch_3')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh'})\n",
        "zip_file.GetContentFile('data_batch_4')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1rmRytfjJWua0cv17DzST6PqoDFY2APa6'})\n",
        "zip_file.GetContentFile('data_batch_5')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp'})\n",
        "zip_file.GetContentFile('test_batch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MqFf8K2m7c4H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data1 = unpickle(\"data_batch_1\")\n",
        "data2 = unpickle(\"data_batch_2\")\n",
        "data3 = unpickle(\"data_batch_3\")\n",
        "data4 = unpickle(\"data_batch_4\")\n",
        "data5 = unpickle(\"data_batch_5\")\n",
        "#label_data = unpickle('../input/batches.meta')[b'label_names']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BTjhS9i-7hzh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels1 = data1[b'labels']\n",
        "data1 = data1[b'data'] * 1.0\n",
        "labels2 = data2[b'labels']\n",
        "data2 = data2[b'data'] * 1.0\n",
        "labels3 = data3[b'labels']\n",
        "data3 = data3[b'data'] * 1.0\n",
        "labels4 = data4[b'labels']\n",
        "data4 = data4[b'data']  * 1.0\n",
        "labels5 = data5[b'labels']\n",
        "data5 = data5[b'data']  * 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dmI1e73001ke",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Combine the remaining four arrays to use as training data"
      ]
    },
    {
      "metadata": {
        "id": "t6J6gMPT7nWU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_tr = np.concatenate([data1, data2, data3, data4, data5], axis=0)\n",
        "X_tr = np.dstack((X_tr[:, :1024], X_tr[:, 1024:2048], X_tr[:, 2048:])) / 1.0\n",
        "X_tr = (X_tr - 128) / 255.0\n",
        "X_tr = X_tr.reshape(-1, 32, 32, 3)\n",
        "\n",
        "y_tr = np.concatenate([labels1, labels2, labels3, labels4, labels5], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xx9HOPh407P0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setting the number of classes"
      ]
    },
    {
      "metadata": {
        "id": "ruZ1VkFv7rey",
        "colab_type": "code",
        "outputId": "76bb79a4-d9c5-4375-f05f-c73a79e696a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "num_classes = len(np.unique(y_tr))\n",
        "\n",
        "print(\"X_tr\", X_tr.shape)\n",
        "print(\"y_tr\", y_tr.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_tr (50000, 32, 32, 3)\n",
            "y_tr (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mB8z9GK71C50",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing the test data"
      ]
    },
    {
      "metadata": {
        "id": "4SoEf6lH7wr2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data = unpickle(\"test_batch\")\n",
        "\n",
        "X_test = test_data[b'data']\n",
        "X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:])) / 1.0\n",
        "X_test = (X_test - 128) / 255.0\n",
        "X_test = X_test.reshape(-1, 32, 32, 3)\n",
        "y_test = np.asarray(test_data[b'labels'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cl0DqbAC1HkJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spliting into test and validation"
      ]
    },
    {
      "metadata": {
        "id": "5vHznevh71KL",
        "colab_type": "code",
        "outputId": "866f43a7-9aff-4b38-cd21-2113235f0e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X_te, X_cv, y_te, y_cv = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
        "\n",
        "print(\"X_te\", X_te.shape)\n",
        "print(\"X_cv\", X_cv.shape)\n",
        "print(\"y_te\", y_te.shape)\n",
        "print(\"y_cv\", y_cv.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_te (5000, 32, 32, 3)\n",
            "X_cv (5000, 32, 32, 3)\n",
            "y_te (5000,)\n",
            "y_cv (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kxWDLv881OtU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch generator"
      ]
    },
    {
      "metadata": {
        "id": "Y-oS6X3q76de",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(X, y, batch_size, crop=False, distort=True):\n",
        "    # Shuffle X,y\n",
        "    shuffled_idx = np.arange(len(y))\n",
        "    np.random.shuffle(shuffled_idx)\n",
        "    i, h, w, c = X.shape\n",
        "    \n",
        "    # Enumerate indexes by steps of batch_size\n",
        "    for i in range(0, len(y), batch_size):\n",
        "        batch_idx = shuffled_idx[i:i+batch_size]\n",
        "        X_return = X[batch_idx]\n",
        "        \n",
        "        # optional random crop of images\n",
        "        if crop:\n",
        "            woff = (w - 24) // 4\n",
        "            hoff = (h - 24) // 4\n",
        "            startw = np.random.randint(low=woff,high=woff*2)\n",
        "            starth = np.random.randint(low=hoff,high=hoff*2)\n",
        "            X_return = X_return[:,startw:startw+24,starth:starth+24,:]\n",
        "       \n",
        "        # do random flipping of images\n",
        "        coin = np.random.binomial(1, 0.5, size=None)\n",
        "        if coin and distort:\n",
        "            X_return = X_return[...,::-1,:]\n",
        "        \n",
        "        yield X_return, y[batch_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VDLatfQ1UoO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ]
    },
    {
      "metadata": {
        "id": "OLz60CwK8CBx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 50                   # how many epochs\n",
        "batch_size = 128\n",
        "steps_per_epoch = X_tr.shape[0] / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kTsErIhw1cdv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the network\n",
        "\n",
        "\n",
        "## MODEL 7.13.4.6.7f\n",
        "Model description:\n",
        "\n",
        "* 7.6 - changed kernel reg rate to 0.01 from 0.1\n",
        "* 7.7 - optimize loss instead of ce 7.8 - remove redundant lambda, replaced scale in regularizer with lambda, changed lambda from 0.01 to 0.001\n",
        "* 7.9 - lambda 0 instead of 3\n",
        "* 7.9.1 - lambda 1 instead of 0\n",
        "* 7.9.2 - use lambda 2 instead of 1\n",
        "* 7.9.4f - use 3x3 pooling instead of 2x2\n",
        "* 7.11.6f - add batch norm after conv 5\n",
        "* 7.11.2f - raise lambda, add dropout after fc2\n",
        "* 7.12.2f - change fully connected dropout to 20%\n",
        "* 7.12.2.2g - change fc dropout to 25%, increase filters in last 2 conv layers to 192 from 128\n",
        "* 7.13.2.2f - change all pool sizes to 2x2 from 3x3\n",
        "* 7.13.3.6f - use different lambda for conv + fc layers"
      ]
    },
    {
      "metadata": {
        "id": "iGCEtfzF8Q7o",
        "colab_type": "code",
        "outputId": "a85797e8-d0fb-4d2d-c103-dcf3a22153e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "cell_type": "code",
      "source": [
        "# Create new graph\n",
        "graph = tf.Graph()\n",
        "# whether to retrain model from scratch or use saved model\n",
        "init = True\n",
        "model_name = \"model_7.13.4.7.7l\"\n",
        "\n",
        "with graph.as_default():\n",
        "    # Placeholders\n",
        "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
        "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
        "    training = tf.placeholder(dtype=tf.bool)\n",
        "    \n",
        "    # create global step for decaying learning rate\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # lambda 6\n",
        "    lamC = 0.000050\n",
        "    lamF = 0.0025000\n",
        "           \n",
        "    # learning rate j\n",
        "    epochs_per_decay = 10\n",
        "    starting_rate = 0.003\n",
        "    decay_factor = 0.9\n",
        "    staircase = True\n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(starting_rate,                 # start at 0.003\n",
        "                                               global_step, \n",
        "                                               steps_per_epoch * epochs_per_decay,       # 100 epochs\n",
        "                                               decay_factor,                   # 0.5 decrease\n",
        "                                               staircase=staircase) \n",
        "    \n",
        "    # Small epsilon value for the BN transform\n",
        "    epsilon = 1e-3\n",
        "    \n",
        "    with tf.name_scope('conv1') as scope:\n",
        "        # Convolutional layer 1 \n",
        "        conv1 = tf.layers.conv2d(\n",
        "            X,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=10),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv1'                 \n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn1 = tf.layers.batch_normalization(\n",
        "            conv1,\n",
        "            axis=-1,\n",
        "            momentum=0.99,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn1'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
        "\n",
        "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=9, training=training)\n",
        "    \n",
        "    with tf.name_scope('conv2') as scope:\n",
        "        # Convolutional layer 2\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            conv1_bn_relu,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=8),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv2'                  # Add name\n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn2 = tf.layers.batch_normalization(\n",
        "            conv2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn2'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
        "    \n",
        "    with tf.name_scope('pool1') as scope:\n",
        "         # Max pooling layer 1\n",
        "        pool1 = tf.layers.max_pooling2d(\n",
        "            conv2_bn_relu,                       # Input\n",
        "            pool_size=(2, 2),            # Pool size: 3x3\n",
        "            strides=(2, 2),              # Stride: 2\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            name='pool1'\n",
        "        )\n",
        "\n",
        "        # dropout at 10%\n",
        "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
        "\n",
        "    with tf.name_scope('conv3') as scope:\n",
        "        # Convolutional layer 3\n",
        "        conv3= tf.layers.conv2d(\n",
        "            pool1,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=7),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv3'                 \n",
        "        )\n",
        "\n",
        "        bn3 = tf.layers.batch_normalization(\n",
        "            conv3,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn3'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=0, training=training)\n",
        "\n",
        "    with tf.name_scope('conv4') as scope:\n",
        "        # Convolutional layer 4\n",
        "        conv4= tf.layers.conv2d(\n",
        "            conv3_bn_relu,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv4'                 \n",
        "        )\n",
        "\n",
        "        bn4 = tf.layers.batch_normalization(\n",
        "            conv4,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn4'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
        "    \n",
        "    # Max pooling layer 2 \n",
        "    pool2 = tf.layers.max_pooling2d(\n",
        "        conv4_bn_relu,                       # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool2'\n",
        "    )\n",
        "\n",
        "    with tf.name_scope('conv5') as scope:\n",
        "        # Convolutional layer 5\n",
        "        conv5= tf.layers.conv2d(\n",
        "            pool2,                       # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv5'                 \n",
        "        )\n",
        "        \n",
        "        \n",
        "        bn5 = tf.layers.batch_normalization(\n",
        "            conv5,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn5'\n",
        "        )\n",
        "        \n",
        "        # activation\n",
        "        conv5_bn_relu = tf.nn.relu(bn5, name='relu5')\n",
        "\n",
        "        # try dropout here\n",
        "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
        "\n",
        "    with tf.name_scope('conv6') as scope:\n",
        "        # Convolutional layer 6\n",
        "        conv6= tf.layers.conv2d(\n",
        "            conv5_bn_relu,               # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv6'                \n",
        "        )\n",
        "\n",
        "        bn6 = tf.layers.batch_normalization(\n",
        "            conv6,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn6'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv6_bn_relu = tf.nn.relu(bn6, name='relu6')\n",
        "    \n",
        "    # Max pooling layer 3\n",
        "    pool3 = tf.layers.max_pooling2d(\n",
        "        conv6_bn_relu,               # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool3'\n",
        "    )\n",
        "    \n",
        "    with tf.name_scope('flatten') as scope:\n",
        "        # Flatten output\n",
        "        flat_output = tf.contrib.layers.flatten(pool3)\n",
        "\n",
        "        # dropout at 10%\n",
        "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
        "    \n",
        "    # Fully connected layer 1\n",
        "    with tf.name_scope('fc1') as scope:\n",
        "        fc1 = tf.layers.dense(\n",
        "            flat_output,                 # input\n",
        "            1024,                        # 1024 hidden units\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc1\"\n",
        "        )\n",
        "        \n",
        "        bn7 = tf.layers.batch_normalization(\n",
        "            fc1,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn7'\n",
        "        )\n",
        "        \n",
        "        fc1_relu = tf.nn.relu(bn7, name='fc1_relu')\n",
        "        \n",
        "        # dropout at 25%\n",
        "        fc1_do = tf.layers.dropout(fc1_relu, rate=0.25, seed=10, training=training)\n",
        "    \n",
        "    # Fully connected layer 2\n",
        "    with tf.name_scope('fc2') as scope:\n",
        "        fc2 = tf.layers.dense(\n",
        "            fc1_do,                        # input\n",
        "            512,                        # 512 hidden units\n",
        "            activation=None,            # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=5),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc2\"\n",
        "        )\n",
        "        \n",
        "        bn8 = tf.layers.batch_normalization(\n",
        "            fc2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn8'\n",
        "        )\n",
        "        \n",
        "        fc2_relu = tf.nn.relu(bn8, name='fc2_relu')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        fc2_do = tf.layers.dropout(fc2_relu, rate=0.25, seed=11, training=training)\n",
        "    \n",
        "    # Output layer\n",
        "    logits = tf.layers.dense(\n",
        "        fc2_do,                         # input\n",
        "        num_classes,                           # One output unit per category\n",
        "        activation=None,             # No activation function\n",
        "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
        "        bias_initializer=tf.zeros_initializer(),\n",
        "        name=\"logits\"\n",
        "    )\n",
        "    \n",
        "    # Kernel weights of the 1st conv. layer\n",
        "    with tf.variable_scope('conv1', reuse=True):\n",
        "        conv_kernels1 = tf.get_variable('kernel')\n",
        "    \n",
        "    # Mean cross-entropy\n",
        "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
        "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
        "    \n",
        "    # Adam optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    \n",
        "    # Minimize cross-entropy\n",
        "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Compute predictions and accuracy\n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    is_correct = tf.equal(y, predictions)\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "    \n",
        "    # add this so that the batch norm gets run\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "    # Create summary hooks\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "    tf.summary.scalar('cross_entropy', mean_ce)\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "    \n",
        "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
        "    merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-17-6638cb78506d>:59: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-17-6638cb78506d>:75: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-17-6638cb78506d>:81: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-17-6638cb78506d>:123: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-17-6638cb78506d>:298: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "haQcV_IR1-J7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CONFIGURE OPTIONS"
      ]
    },
    {
      "metadata": {
        "id": "QXwBYl-l8Wza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = True                   # whether to initialize the model or use a saved version\n",
        "crop = False                  # do random cropping of images?\n",
        "\n",
        "meta_data_every = 5\n",
        "log_to_tensorboard = False\n",
        "print_every = 1                # how often to print metrics\n",
        "checkpoint_every = 1           # how often to save model in epochs\n",
        "use_gpu = True                 # whether or not to use the GPU\n",
        "print_metrics = True          # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
        "\n",
        "# Placeholders for metrics\n",
        "if init:\n",
        "    valid_acc_values = []\n",
        "    valid_cost_values = []\n",
        "    train_acc_values = []\n",
        "    train_cost_values = []\n",
        "    train_lr_values = []\n",
        "    train_loss_values = []\n",
        "    \n",
        "\n",
        "config = tf.ConfigProto()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uX1DBzHE2AH4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trainig the model"
      ]
    },
    {
      "metadata": {
        "id": "DkkpQUUz8eRI",
        "colab_type": "code",
        "outputId": "e3a07e3d-e73a-4ce6-e812-12f5bc460e9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    if log_to_tensorboard:\n",
        "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
        "    \n",
        "    if not print_metrics:\n",
        "        # create a plot to be updated as model is trained\n",
        "        f, ax = plt.subplots(1,3,figsize=(20,5))\n",
        "    \n",
        "    # create the saver\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # If the model is new initialize variables, else restore the session\n",
        "    if init:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "    else:\n",
        "        saver.restore(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "\n",
        "    # Set seed\n",
        "    np.random.seed(0)\n",
        "    \n",
        "    print(\"Training\", model_name, \"...\")\n",
        "    \n",
        "    # Train several epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Accuracy values (train) after each batch\n",
        "        batch_acc = []\n",
        "        batch_cost = []\n",
        "        batch_loss = []\n",
        "        batch_lr = []\n",
        "        \n",
        "        # only log run metadata once per epoch\n",
        "        write_meta_data = False\n",
        "            \n",
        "        for X_batch, y_batch in get_batches(X_tr, y_tr, batch_size, crop=crop, distort=True):\n",
        "            if write_meta_data and log_to_tensboard:\n",
        "                # create the metadata\n",
        "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                run_metadata = tf.RunMetadata()\n",
        "            \n",
        "                # Run training and evaluate accuracy\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                },\n",
        "                options=run_options,\n",
        "                run_metadata=run_metadata)\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                train_writer.add_run_metadata(run_metadata, 'step %d' % step)\n",
        "                train_writer.add_summary(summary, step)\n",
        "                write_meta_data = False\n",
        "                \n",
        "            else:\n",
        "                # Run training without meta data\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                })\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                if log_to_tensorboard:\n",
        "                    train_writer.add_summary(summary, step)\n",
        "\n",
        "        # save checkpoint every nth epoch\n",
        "        if(epoch % checkpoint_every == 0):\n",
        "            print(\"Saving checkpoint\")\n",
        "            # save the model\n",
        "            save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "            # Now that model is saved set init to false so we reload it\n",
        "            init = False\n",
        "        \n",
        "        # init batch arrays\n",
        "        batch_cv_acc = []\n",
        "        batch_cv_cost = []\n",
        "        batch_cv_loss = []\n",
        "        \n",
        "        # Evaluate validation accuracy with batches so as to not crash the GPU\n",
        "        for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size, crop=crop, distort=False):\n",
        "            summary, valid_acc, valid_cost, valid_loss = sess.run([merged, accuracy, mean_ce, loss], feed_dict={\n",
        "                X: X_batch,\n",
        "                y: y_batch,\n",
        "                training: False\n",
        "            })\n",
        "\n",
        "            batch_cv_acc.append(valid_acc)\n",
        "            batch_cv_cost.append(valid_cost)\n",
        "            batch_cv_loss.append(valid_loss)\n",
        "\n",
        "        # Write average of validation data to summary logs\n",
        "        if log_to_tensorboard:\n",
        "            summary = tf.Summary(value=[tf.Summary.Value(tag=\"accuracy\", simple_value=np.mean(batch_cv_acc)),tf.Summary.Value(tag=\"cross_entropy\", simple_value=np.mean(batch_cv_cost)),])\n",
        "            test_writer.add_summary(summary, step)\n",
        "            step += 1\n",
        "            \n",
        "        # take the mean of the values to add to the metrics\n",
        "        valid_acc_values.append(np.mean(batch_cv_acc))\n",
        "        valid_cost_values.append(np.mean(batch_cv_cost))\n",
        "        train_acc_values.append(np.mean(batch_acc))\n",
        "        train_cost_values.append(np.mean(batch_cost))\n",
        "        train_lr_values.append(np.mean(batch_lr))\n",
        "        train_loss_values.append(np.mean(batch_loss))\n",
        "        \n",
        "        if print_metrics:\n",
        "            # Print progress every nth epoch to keep output to reasonable amount\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))\n",
        "        else:\n",
        "            # update the plot\n",
        "            ax[0].cla()\n",
        "            ax[0].plot(valid_acc_values, color=\"red\", label=\"Validation\")\n",
        "            ax[0].plot(train_acc_values, color=\"blue\", label=\"Training\")\n",
        "            ax[0].set_title('Validation accuracy: {:.4f} (mean last 3)'.format(np.mean(valid_acc_values[-3:])))\n",
        "            \n",
        "            # since we can't zoom in on plots like in tensorboard, scale y axis to give a decent amount of detail\n",
        "            if np.mean(valid_acc_values[-3:]) > 0.85:\n",
        "                ax[0].set_ylim([0.75,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.75:\n",
        "                ax[0].set_ylim([0.65,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.65:\n",
        "                ax[0].set_ylim([0.55,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.55:\n",
        "                ax[0].set_ylim([0.45,1.0])           \n",
        "            \n",
        "            ax[0].set_xlabel('Epoch')\n",
        "            ax[0].set_ylabel('Accuracy')\n",
        "            ax[0].legend()\n",
        "            \n",
        "            ax[1].cla()\n",
        "            ax[1].plot(valid_cost_values, color=\"red\", label=\"Validation\")\n",
        "            ax[1].plot(train_cost_values, color=\"blue\", label=\"Training\")\n",
        "            ax[1].set_title('Validation xentropy: {:.3f} (mean last 3)'.format(np.mean(valid_cost_values[-3:])))\n",
        "            ax[1].set_xlabel('Epoch')\n",
        "            ax[1].set_ylabel('Cross Entropy')\n",
        "            ax[1].legend()\n",
        "            \n",
        "            ax[2].cla()\n",
        "            ax[2].plot(train_lr_values)\n",
        "            ax[2].set_title(\"Learning rate: {:.6f}\".format(np.mean(train_lr_values[-1:])))\n",
        "            ax[2].set_xlabel(\"Epoch\")\n",
        "            ax[2].set_ylabel(\"Learning Rate\")\n",
        "            \n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "            \n",
        "        # Print data every 50th epoch so I can write it down to compare models\n",
        "        if (not print_metrics) and (epoch % 50 == 0) and (epoch > 1):\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))  \n",
        "            \n",
        "    # print results of last epoch\n",
        "    print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "                epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "            ))\n",
        "    \n",
        "    # save the session\n",
        "    save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "    # init the test data array\n",
        "    test_acc_values = []\n",
        "    \n",
        "    # Check on the test data\n",
        "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, crop=crop, distort=False):\n",
        "        test_accuracy = sess.run(accuracy, feed_dict={\n",
        "            X: X_batch,\n",
        "            y: y_batch,\n",
        "            training: False\n",
        "        })\n",
        "        test_acc_values.append(test_accuracy)\n",
        "    \n",
        "    # average test accuracy across batches\n",
        "    test_acc = np.mean(test_acc_values)\n",
        "    \n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "# print results of last epoch\n",
        "print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "      epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model_7.13.4.7.7l ...\n",
            "Saving checkpoint\n",
            "Epoch 00 - step 391 - cv acc: 0.430 - train acc: 0.494 (mean) - cv cost: 1.655 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 01 - step 782 - cv acc: 0.623 - train acc: 0.652 (mean) - cv cost: 1.087 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 02 - step 1173 - cv acc: 0.728 - train acc: 0.711 (mean) - cv cost: 0.824 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 03 - step 1564 - cv acc: 0.748 - train acc: 0.745 (mean) - cv cost: 0.742 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 04 - step 1955 - cv acc: 0.768 - train acc: 0.769 (mean) - cv cost: 0.677 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 05 - step 2346 - cv acc: 0.789 - train acc: 0.787 (mean) - cv cost: 0.641 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 06 - step 2737 - cv acc: 0.790 - train acc: 0.797 (mean) - cv cost: 0.639 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 07 - step 3128 - cv acc: 0.786 - train acc: 0.814 (mean) - cv cost: 0.663 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 08 - step 3519 - cv acc: 0.806 - train acc: 0.822 (mean) - cv cost: 0.609 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 09 - step 3910 - cv acc: 0.825 - train acc: 0.831 (mean) - cv cost: 0.566 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 10 - step 4301 - cv acc: 0.824 - train acc: 0.841 (mean) - cv cost: 0.557 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 11 - step 4692 - cv acc: 0.800 - train acc: 0.850 (mean) - cv cost: 0.621 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 12 - step 5083 - cv acc: 0.837 - train acc: 0.857 (mean) - cv cost: 0.531 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 13 - step 5474 - cv acc: 0.826 - train acc: 0.860 (mean) - cv cost: 0.505 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 14 - step 5865 - cv acc: 0.841 - train acc: 0.860 (mean) - cv cost: 0.489 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 15 - step 6256 - cv acc: 0.834 - train acc: 0.869 (mean) - cv cost: 0.499 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 16 - step 6647 - cv acc: 0.860 - train acc: 0.871 (mean) - cv cost: 0.444 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 17 - step 7038 - cv acc: 0.836 - train acc: 0.872 (mean) - cv cost: 0.507 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 18 - step 7429 - cv acc: 0.851 - train acc: 0.876 (mean) - cv cost: 0.470 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 19 - step 7820 - cv acc: 0.832 - train acc: 0.881 (mean) - cv cost: 0.533 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 20 - step 8211 - cv acc: 0.850 - train acc: 0.887 (mean) - cv cost: 0.457 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 21 - step 8602 - cv acc: 0.867 - train acc: 0.889 (mean) - cv cost: 0.413 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 22 - step 8993 - cv acc: 0.856 - train acc: 0.893 (mean) - cv cost: 0.450 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 23 - step 9384 - cv acc: 0.862 - train acc: 0.893 (mean) - cv cost: 0.430 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 24 - step 9775 - cv acc: 0.849 - train acc: 0.895 (mean) - cv cost: 0.478 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 25 - step 10166 - cv acc: 0.874 - train acc: 0.896 (mean) - cv cost: 0.397 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 26 - step 10557 - cv acc: 0.854 - train acc: 0.895 (mean) - cv cost: 0.462 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 27 - step 10948 - cv acc: 0.867 - train acc: 0.900 (mean) - cv cost: 0.412 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 28 - step 11339 - cv acc: 0.855 - train acc: 0.901 (mean) - cv cost: 0.470 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 29 - step 11730 - cv acc: 0.862 - train acc: 0.903 (mean) - cv cost: 0.442 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 30 - step 12121 - cv acc: 0.851 - train acc: 0.907 (mean) - cv cost: 0.452 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 31 - step 12512 - cv acc: 0.864 - train acc: 0.909 (mean) - cv cost: 0.443 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 32 - step 12903 - cv acc: 0.864 - train acc: 0.911 (mean) - cv cost: 0.425 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 33 - step 13294 - cv acc: 0.877 - train acc: 0.912 (mean) - cv cost: 0.388 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 34 - step 13685 - cv acc: 0.859 - train acc: 0.912 (mean) - cv cost: 0.450 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 35 - step 14076 - cv acc: 0.875 - train acc: 0.915 (mean) - cv cost: 0.422 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 36 - step 14467 - cv acc: 0.870 - train acc: 0.914 (mean) - cv cost: 0.429 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 37 - step 14858 - cv acc: 0.873 - train acc: 0.913 (mean) - cv cost: 0.408 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 38 - step 15249 - cv acc: 0.875 - train acc: 0.917 (mean) - cv cost: 0.405 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 39 - step 15640 - cv acc: 0.876 - train acc: 0.915 (mean) - cv cost: 0.406 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 40 - step 16031 - cv acc: 0.876 - train acc: 0.923 (mean) - cv cost: 0.411 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 41 - step 16422 - cv acc: 0.877 - train acc: 0.922 (mean) - cv cost: 0.408 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 42 - step 16813 - cv acc: 0.864 - train acc: 0.925 (mean) - cv cost: 0.444 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 43 - step 17204 - cv acc: 0.878 - train acc: 0.924 (mean) - cv cost: 0.409 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 44 - step 17595 - cv acc: 0.871 - train acc: 0.924 (mean) - cv cost: 0.421 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 45 - step 17986 - cv acc: 0.870 - train acc: 0.926 (mean) - cv cost: 0.445 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 46 - step 18377 - cv acc: 0.875 - train acc: 0.925 (mean) - cv cost: 0.411 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 47 - step 18768 - cv acc: 0.876 - train acc: 0.927 (mean) - cv cost: 0.405 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 48 - step 19159 - cv acc: 0.868 - train acc: 0.925 (mean) - cv cost: 0.428 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 49 - step 19550 - cv acc: 0.881 - train acc: 0.927 (mean) - cv cost: 0.372 - lr: 0.00177\n",
            "Epoch 50 - cv acc: 0.8811 - train acc: 0.9272 (mean) - cv cost: 0.372\n",
            "Epoch 50 - cv acc: 0.8811 - train acc: 0.9272 (mean) - cv cost: 0.372\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fv4sbzAL2Wmm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Scoring and Evaluating trained model\n"
      ]
    },
    {
      "metadata": {
        "id": "xrrWDX5CGPFr",
        "colab_type": "code",
        "outputId": "8c6e2e92-2fe4-4a0f-b922-5a9ef7b6d98d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## MODEL 7.20.0.11g \n",
        "print(\"Model : \", model_name)\n",
        "\n",
        "print(\"Convolutional network accuracy (test set):\",test_acc, \" Validation Set\", valid_acc_values[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model :  model_7.13.4.7.7l\n",
            "Convolutional network accuracy (test set): 0.88027346  Validation Set 0.8810547\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a-SVPpbFPj2g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w_liwig82ajP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "The CIFAR-10 dataset for image classification model using convolutional neural network gave an accuracy of 88.% for 50 epoochs. This was the model in which the parameters used were: \n",
        "* Activation Function: Rectified linear unit (ReLU) \n",
        "* Cost function: Cross-Entropy \n",
        "* No.of Epochs: 50 \n",
        "* Gradient estimation: ADAM \n",
        "* Network Architecture:Number of layers: 12\n",
        "* Network initialization: zero\n",
        "\n",
        "I ran it for 50 epochs and got almost 88% accuracy. It can surely go much further since it was still undertrained! To improve the model the learning rate should be improved. Also, number of neurons can be more complicated for a better fit.Along, with that all, the parameters mentioned above can also be changed in order to improve the model accuracy.\n",
        "\n",
        "# References\n",
        "[1] https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
        "\n",
        "[2] http://www.cs.utoronto.ca/~kriz/cifar.html\n",
        "\n",
        "[3] https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c\n",
        "\n",
        "[4] https://www.kaggle.com/skooch/cifar-10-in-tensorflow/notebook"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_10tensorflow(Model 1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "QKhzcxyO0q6o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 3\n",
        "Using Tensorflow to build a CNN network for CIFAR-10 dataset. Each record is of size 1*3072. Building a CNN network to classify the data into the 10 classes.\n",
        "\n",
        "# Dataset\n",
        "CIFAR-10 dataset The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "http://www.cs.utoronto.ca/~kriz/cifar.html\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "bWsbAwnH05c4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installing pydrive"
      ]
    },
    {
      "metadata": {
        "id": "e9yLdyC3okNx",
        "colab_type": "code",
        "outputId": "bf294763-768a-4ea6-fca7-7e66d122d6cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 18.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "masdkeWR1GqT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Creates connection "
      ]
    },
    {
      "metadata": {
        "id": "5k5RJRnzqvF3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import tensorflow as tf\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1fvoe1u1Nzy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticating and creating the PyDrive client"
      ]
    },
    {
      "metadata": {
        "id": "I3GaF045qyGc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bAfCJsc_1YAu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting ids of all the files in folder"
      ]
    },
    {
      "metadata": {
        "id": "5zDV8e4UrMF0",
        "colab_type": "code",
        "outputId": "f23097fe-3f71-4a40-b616-32987741861c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1DCFFw2O6BFq8Gk0eYu7JT4Qn224BNoCt' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: data_batch_1, id: 11Bo2ULl9_aOQ761ONc2vhepnydriELiT\n",
            "title: data_batch_2, id: 1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I\n",
            "title: test_batch, id: 1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp\n",
            "title: data_batch_3, id: 11ky6i6FSTGWJYOzXquELD4H-GUr49C4f\n",
            "title: data_batch_5, id: 1rmRytfjJWua0cv17DzST6PqoDFY2APa6\n",
            "title: data_batch_4, id: 1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "w7WxeIKl1C5_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "metadata": {
        "id": "xrCxjdpcsdfD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jxl57YGs1ida",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "iBPEy2KgseGq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nndfkydh1oNz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#if file is zipped"
      ]
    },
    {
      "metadata": {
        "id": "3C2clM7GtEn-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_file = drive.CreateFile({'id': '11Bo2ULl9_aOQ761ONc2vhepnydriELiT'})\n",
        "zip_file.GetContentFile('data_batch_1')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I'})\n",
        "zip_file.GetContentFile('data_batch_2')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '11ky6i6FSTGWJYOzXquELD4H-GUr49C4f'})\n",
        "zip_file.GetContentFile('data_batch_3')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh'})\n",
        "zip_file.GetContentFile('data_batch_4')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1rmRytfjJWua0cv17DzST6PqoDFY2APa6'})\n",
        "zip_file.GetContentFile('data_batch_5')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp'})\n",
        "zip_file.GetContentFile('test_batch')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bSBmWrQ1swp_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data1 = unpickle(\"data_batch_1\")\n",
        "data2 = unpickle(\"data_batch_2\")\n",
        "data3 = unpickle(\"data_batch_3\")\n",
        "data4 = unpickle(\"data_batch_4\")\n",
        "data5 = unpickle(\"data_batch_5\")\n",
        "#label_data = unpickle('../input/batches.meta')[b'label_names']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vISWOPTPvu0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels1 = data1[b'labels']\n",
        "data1 = data1[b'data'] * 1.0\n",
        "labels2 = data2[b'labels']\n",
        "data2 = data2[b'data'] * 1.0\n",
        "labels3 = data3[b'labels']\n",
        "data3 = data3[b'data'] * 1.0\n",
        "labels4 = data4[b'labels']\n",
        "data4 = data4[b'data']  * 1.0\n",
        "labels5 = data5[b'labels']\n",
        "data5 = data5[b'data']  * 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5IxKY-rL1wQP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Combine the remaining four arrays to use as training data"
      ]
    },
    {
      "metadata": {
        "id": "QZWZ_lAOw7wQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_tr = np.concatenate([data1, data2, data3, data4, data5], axis=0)\n",
        "X_tr = np.dstack((X_tr[:, :1024], X_tr[:, 1024:2048], X_tr[:, 2048:])) / 1.0\n",
        "X_tr = (X_tr - 128) / 255.0\n",
        "X_tr = X_tr.reshape(-1, 32, 32, 3)\n",
        "\n",
        "y_tr = np.concatenate([labels1, labels2, labels3, labels4, labels5], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1LKi9mM30uW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setting the number of classes"
      ]
    },
    {
      "metadata": {
        "id": "2rPn-CJTw_wq",
        "colab_type": "code",
        "outputId": "535565bf-16ee-4d57-901b-0fd4beca3d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "num_classes = len(np.unique(y_tr))\n",
        "\n",
        "print(\"X_tr\", X_tr.shape)\n",
        "print(\"y_tr\", y_tr.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_tr (50000, 32, 32, 3)\n",
            "y_tr (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XROPuiRe38nT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing the test data"
      ]
    },
    {
      "metadata": {
        "id": "Co5-_wXzxCyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data = unpickle(\"test_batch\")\n",
        "\n",
        "X_test = test_data[b'data']\n",
        "X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:])) / 1.0\n",
        "X_test = (X_test - 128) / 255.0\n",
        "X_test = X_test.reshape(-1, 32, 32, 3)\n",
        "y_test = np.asarray(test_data[b'labels'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tXnRizan4BEd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spliting into test and validation"
      ]
    },
    {
      "metadata": {
        "id": "bpaNc3-VxQdp",
        "colab_type": "code",
        "outputId": "661d92c7-0a6c-477c-fd81-58a0d942f6e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X_te, X_cv, y_te, y_cv = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
        "\n",
        "print(\"X_te\", X_te.shape)\n",
        "print(\"X_cv\", X_cv.shape)\n",
        "print(\"y_te\", y_te.shape)\n",
        "print(\"y_cv\", y_cv.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_te (5000, 32, 32, 3)\n",
            "X_cv (5000, 32, 32, 3)\n",
            "y_te (5000,)\n",
            "y_cv (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F4c81ZLP4Gta",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch generator"
      ]
    },
    {
      "metadata": {
        "id": "fAn8XmbhxYnV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(X, y, batch_size, crop=False, distort=True):\n",
        "    # Shuffle X,y\n",
        "    shuffled_idx = np.arange(len(y))\n",
        "    np.random.shuffle(shuffled_idx)\n",
        "    i, h, w, c = X.shape\n",
        "    \n",
        "    # Enumerate indexes by steps of batch_size\n",
        "    for i in range(0, len(y), batch_size):\n",
        "        batch_idx = shuffled_idx[i:i+batch_size]\n",
        "        X_return = X[batch_idx]\n",
        "        \n",
        "        # optional random crop of images\n",
        "        if crop:\n",
        "            woff = (w - 24) // 4\n",
        "            hoff = (h - 24) // 4\n",
        "            startw = np.random.randint(low=woff,high=woff*2)\n",
        "            starth = np.random.randint(low=hoff,high=hoff*2)\n",
        "            X_return = X_return[:,startw:startw+24,starth:starth+24,:]\n",
        "       \n",
        "        # do random flipping of images\n",
        "        coin = np.random.binomial(1, 0.5, size=None)\n",
        "        if coin and distort:\n",
        "            X_return = X_return[...,::-1,:]\n",
        "        \n",
        "        yield X_return, y[batch_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k7GhY-L34T46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the network\n",
        "###  MODEL 7.13.4.6.7f\n",
        "Model description:\n",
        "\n",
        "\n",
        "* 7.6 - changed kernel reg rate to 0.01 from 0.1\n",
        "* 7.7 - optimize loss instead of ce\n",
        "* 7.8 - remove redundant lambda, replaced scale in regularizer with lambda, changed lambda from 0.01 to 0.001\n",
        "* 7.9 - lambda 0 instead of 3\n",
        "* 7.9.1 - lambda 1 instead of 0\n",
        "* 7.9.2 - use lambda 2 instead of 1\n",
        "* 7.9.4f - use 3x3 pooling instead of 2x2\n",
        "* 7.11.6f - add batch norm after conv 5\n",
        "* 7.12.2.2g - change fc dropout to 25%, increase filters in last 2 conv layers to 192 from 128\n",
        "* 7.13.2.2f - change all pool sizes to 2x2 from 3x3\n",
        "* 7.13.3.6f - use different lambda for conv + fc layers"
      ]
    },
    {
      "metadata": {
        "id": "6sDTox85xgDm",
        "colab_type": "code",
        "outputId": "7ee440da-edaf-4835-f36d-b6ddf8566101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "cell_type": "code",
      "source": [
        "# Create new graph\n",
        "graph = tf.Graph()\n",
        "# whether to retrain model from scratch or use saved model\n",
        "init = True\n",
        "model_name = \"model_7.13.4.7.7l\"\n",
        "\n",
        "\n",
        "\n",
        "with graph.as_default():\n",
        "    # Placeholders\n",
        "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
        "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
        "    training = tf.placeholder(dtype=tf.bool)\n",
        "    \n",
        "    # create global step for decaying learning rate\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # lambda 6\n",
        "    lamC = 0.000050\n",
        "    lamF = 0.0025000\n",
        "           \n",
        "    # learning rate j\n",
        "    epochs_per_decay = 10\n",
        "    starting_rate = 0.003\n",
        "    decay_factor = 0.9\n",
        "    staircase = True\n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(starting_rate,                 # start at 0.003\n",
        "                                               global_step, \n",
        "                                               steps_per_epoch * epochs_per_decay,       # 100 epochs\n",
        "                                               decay_factor,                   # 0.5 decrease\n",
        "                                               staircase=staircase) \n",
        "    \n",
        "    # Small epsilon value for the BN transform\n",
        "    epsilon = 1e-3\n",
        "    \n",
        "    with tf.name_scope('conv1') as scope:\n",
        "        # Convolutional layer 1 \n",
        "        conv1 = tf.layers.conv2d(\n",
        "            X,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=10),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv1'                 \n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn1 = tf.layers.batch_normalization(\n",
        "            conv1,\n",
        "            axis=-1,\n",
        "            momentum=0.99,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn1'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
        "\n",
        "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=9, training=training)\n",
        "    \n",
        "    with tf.name_scope('conv2') as scope:\n",
        "        # Convolutional layer 2\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            conv1_bn_relu,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=8),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv2'                  # Add name\n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn2 = tf.layers.batch_normalization(\n",
        "            conv2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn2'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
        "    \n",
        "    with tf.name_scope('pool1') as scope:\n",
        "         # Max pooling layer 1\n",
        "        pool1 = tf.layers.max_pooling2d(\n",
        "            conv2_bn_relu,                       # Input\n",
        "            pool_size=(2, 2),            # Pool size: 3x3\n",
        "            strides=(2, 2),              # Stride: 2\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            name='pool1'\n",
        "        )\n",
        "\n",
        "        # dropout at 10%\n",
        "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
        "\n",
        "    with tf.name_scope('conv3') as scope:\n",
        "        # Convolutional layer 3\n",
        "        conv3= tf.layers.conv2d(\n",
        "            pool1,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=7),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv3'                 \n",
        "        )\n",
        "\n",
        "        bn3 = tf.layers.batch_normalization(\n",
        "            conv3,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn3'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=0, training=training)\n",
        "\n",
        "    with tf.name_scope('conv4') as scope:\n",
        "        # Convolutional layer 4\n",
        "        conv4= tf.layers.conv2d(\n",
        "            conv3_bn_relu,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv4'                 \n",
        "        )\n",
        "\n",
        "        bn4 = tf.layers.batch_normalization(\n",
        "            conv4,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn4'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
        "    \n",
        "    # Max pooling layer 2 \n",
        "    pool2 = tf.layers.max_pooling2d(\n",
        "        conv4_bn_relu,                       # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool2'\n",
        "    )\n",
        "\n",
        "    with tf.name_scope('conv5') as scope:\n",
        "        # Convolutional layer 5\n",
        "        conv5= tf.layers.conv2d(\n",
        "            pool2,                       # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv5'                 \n",
        "        )\n",
        "        \n",
        "        \n",
        "        bn5 = tf.layers.batch_normalization(\n",
        "            conv5,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn5'\n",
        "        )\n",
        "        \n",
        "        # activation\n",
        "        conv5_bn_relu = tf.nn.relu(bn5, name='relu5')\n",
        "\n",
        "        # try dropout here\n",
        "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
        "\n",
        "  \n",
        "   \n",
        "    with tf.name_scope('flatten') as scope:\n",
        "        # Flatten output\n",
        "        flat_output = tf.contrib.layers.flatten(pool3)\n",
        "\n",
        "        # dropout at 10%\n",
        "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
        "    \n",
        "    # Fully connected layer 1\n",
        "    with tf.name_scope('fc1') as scope:\n",
        "        fc1 = tf.layers.dense(\n",
        "            flat_output,                 # input\n",
        "            1024,                        # 1024 hidden units\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc1\"\n",
        "        )\n",
        "        \n",
        "        bn6 = tf.layers.batch_normalization(\n",
        "            fc1,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn6'\n",
        "        )\n",
        "        \n",
        "        fc1_relu = tf.nn.relu(bn6, name='fc1_relu')\n",
        "        \n",
        "        # dropout at 25%\n",
        "        fc1_do = tf.layers.dropout(fc1_relu, rate=0.25, seed=10, training=training)\n",
        "    \n",
        "    # Fully connected layer 2\n",
        "    with tf.name_scope('fc2') as scope:\n",
        "        fc2 = tf.layers.dense(\n",
        "            fc1_do,                        # input\n",
        "            512,                        # 512 hidden units\n",
        "            activation=None,            # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=5),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc2\"\n",
        "        )\n",
        "        \n",
        "        bn7 = tf.layers.batch_normalization(\n",
        "            fc2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn7'\n",
        "        )\n",
        "        \n",
        "        fc2_relu = tf.nn.relu(bn7, name='fc2_relu')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        fc2_do = tf.layers.dropout(fc2_relu, rate=0.25, seed=11, training=training)\n",
        "    \n",
        "    # Output layer\n",
        "    logits = tf.layers.dense(\n",
        "        fc2_do,                         # input\n",
        "        num_classes,                           # One output unit per category\n",
        "        activation=None,             # No activation function\n",
        "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
        "        bias_initializer=tf.zeros_initializer(),\n",
        "        name=\"logits\"\n",
        "    )\n",
        "    \n",
        "    # Kernel weights of the 1st conv. layer\n",
        "    with tf.variable_scope('conv1', reuse=True):\n",
        "        conv_kernels1 = tf.get_variable('kernel')\n",
        "    \n",
        "    # Mean cross-entropy\n",
        "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
        "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
        "    \n",
        "    # Adam optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    \n",
        "    # Minimize cross-entropy\n",
        "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Compute predictions and accuracy\n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    is_correct = tf.equal(y, predictions)\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "    \n",
        "    # add this so that the batch norm gets run\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "    # Create summary hooks\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "    tf.summary.scalar('cross_entropy', mean_ce)\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "    \n",
        "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
        "    merged = tf.summary.merge_all()\n",
        "## CONFIGURE OPTIONS\n",
        "init = True                   # whether to initialize the model or use a saved version\n",
        "crop = False                  # do random cropping of images?\n",
        "\n",
        "meta_data_every = 5\n",
        "log_to_tensorboard = False\n",
        "print_every = 1                # how often to print metrics\n",
        "checkpoint_every = 1           # how often to save model in epochs\n",
        "use_gpu = True                 # whether or not to use the GPU\n",
        "print_metrics = True          # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
        "\n",
        "# Placeholders for metrics\n",
        "if init:\n",
        "    valid_acc_values = []\n",
        "    valid_cost_values = []\n",
        "    train_acc_values = []\n",
        "    train_cost_values = []\n",
        "    train_lr_values = []\n",
        "    train_loss_values = []\n",
        "    \n",
        "\n",
        "config = tf.ConfigProto()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-18-d28fd99c74d6>:59: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-18-d28fd99c74d6>:75: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-18-d28fd99c74d6>:81: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-18-d28fd99c74d6>:123: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-18-d28fd99c74d6>:298: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wuVlbNnx58qY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trainig the model"
      ]
    },
    {
      "metadata": {
        "id": "mxZ_-mkoyD-k",
        "colab_type": "code",
        "outputId": "54486a27-51ff-449e-a6a1-360f773c9f23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    if log_to_tensorboard:\n",
        "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
        "    \n",
        "    if not print_metrics:\n",
        "        # create a plot to be updated as model is trained\n",
        "        f, ax = plt.subplots(1,3,figsize=(20,5))\n",
        "    \n",
        "    # create the saver\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # If the model is new initialize variables, else restore the session\n",
        "    if init:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "    else:\n",
        "        saver.restore(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "\n",
        "    # Set seed\n",
        "    np.random.seed(0)\n",
        "    \n",
        "    print(\"Training\", model_name, \"...\")\n",
        "    \n",
        "    # Train several epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Accuracy values (train) after each batch\n",
        "        batch_acc = []\n",
        "        batch_cost = []\n",
        "        batch_loss = []\n",
        "        batch_lr = []\n",
        "        \n",
        "        # only log run metadata once per epoch\n",
        "        write_meta_data = False\n",
        "            \n",
        "        for X_batch, y_batch in get_batches(X_tr, y_tr, batch_size, crop=crop, distort=True):\n",
        "            if write_meta_data and log_to_tensboard:\n",
        "                # create the metadata\n",
        "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                run_metadata = tf.RunMetadata()\n",
        "            \n",
        "                # Run training and evaluate accuracy\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                },\n",
        "                options=run_options,\n",
        "                run_metadata=run_metadata)\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                train_writer.add_run_metadata(run_metadata, 'step %d' % step)\n",
        "                train_writer.add_summary(summary, step)\n",
        "                write_meta_data = False\n",
        "                \n",
        "            else:\n",
        "                # Run training without meta data\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                })\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                if log_to_tensorboard:\n",
        "                    train_writer.add_summary(summary, step)\n",
        "\n",
        "        # save checkpoint every nth epoch\n",
        "        if(epoch % checkpoint_every == 0):\n",
        "            print(\"Saving checkpoint\")\n",
        "            # save the model\n",
        "            save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "            # Now that model is saved set init to false so we reload it\n",
        "            init = False\n",
        "        \n",
        "        # init batch arrays\n",
        "        batch_cv_acc = []\n",
        "        batch_cv_cost = []\n",
        "        batch_cv_loss = []\n",
        "        \n",
        "        # Evaluate validation accuracy with batches so as to not crash the GPU\n",
        "        for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size, crop=crop, distort=False):\n",
        "            summary, valid_acc, valid_cost, valid_loss = sess.run([merged, accuracy, mean_ce, loss], feed_dict={\n",
        "                X: X_batch,\n",
        "                y: y_batch,\n",
        "                training: False\n",
        "            })\n",
        "\n",
        "            batch_cv_acc.append(valid_acc)\n",
        "            batch_cv_cost.append(valid_cost)\n",
        "            batch_cv_loss.append(valid_loss)\n",
        "\n",
        "        # Write average of validation data to summary logs\n",
        "        if log_to_tensorboard:\n",
        "            summary = tf.Summary(value=[tf.Summary.Value(tag=\"accuracy\", simple_value=np.mean(batch_cv_acc)),tf.Summary.Value(tag=\"cross_entropy\", simple_value=np.mean(batch_cv_cost)),])\n",
        "            test_writer.add_summary(summary, step)\n",
        "            step += 1\n",
        "            \n",
        "        # take the mean of the values to add to the metrics\n",
        "        valid_acc_values.append(np.mean(batch_cv_acc))\n",
        "        valid_cost_values.append(np.mean(batch_cv_cost))\n",
        "        train_acc_values.append(np.mean(batch_acc))\n",
        "        train_cost_values.append(np.mean(batch_cost))\n",
        "        train_lr_values.append(np.mean(batch_lr))\n",
        "        train_loss_values.append(np.mean(batch_loss))\n",
        "        \n",
        "        if print_metrics:\n",
        "            # Print progress every nth epoch to keep output to reasonable amount\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))\n",
        "        else:\n",
        "            # update the plot\n",
        "            ax[0].cla()\n",
        "            ax[0].plot(valid_acc_values, color=\"red\", label=\"Validation\")\n",
        "            ax[0].plot(train_acc_values, color=\"blue\", label=\"Training\")\n",
        "            ax[0].set_title('Validation accuracy: {:.4f} (mean last 3)'.format(np.mean(valid_acc_values[-3:])))\n",
        "            \n",
        "            # since we can't zoom in on plots like in tensorboard, scale y axis to give a decent amount of detail\n",
        "            if np.mean(valid_acc_values[-3:]) > 0.85:\n",
        "                ax[0].set_ylim([0.75,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.75:\n",
        "                ax[0].set_ylim([0.65,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.65:\n",
        "                ax[0].set_ylim([0.55,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.55:\n",
        "                ax[0].set_ylim([0.45,1.0])           \n",
        "            \n",
        "            ax[0].set_xlabel('Epoch')\n",
        "            ax[0].set_ylabel('Accuracy')\n",
        "            ax[0].legend()\n",
        "            \n",
        "            ax[1].cla()\n",
        "            ax[1].plot(valid_cost_values, color=\"red\", label=\"Validation\")\n",
        "            ax[1].plot(train_cost_values, color=\"blue\", label=\"Training\")\n",
        "            ax[1].set_title('Validation xentropy: {:.3f} (mean last 3)'.format(np.mean(valid_cost_values[-3:])))\n",
        "            ax[1].set_xlabel('Epoch')\n",
        "            ax[1].set_ylabel('Cross Entropy')\n",
        "            ax[1].legend()\n",
        "            \n",
        "            ax[2].cla()\n",
        "            ax[2].plot(train_lr_values)\n",
        "            ax[2].set_title(\"Learning rate: {:.6f}\".format(np.mean(train_lr_values[-1:])))\n",
        "            ax[2].set_xlabel(\"Epoch\")\n",
        "            ax[2].set_ylabel(\"Learning Rate\")\n",
        "            \n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "            \n",
        "        # Print data every 50th epoch so I can write it down to compare models\n",
        "        if (not print_metrics) and (epoch % 50 == 0) and (epoch > 1):\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))  \n",
        "            \n",
        "    # print results of last epoch\n",
        "    print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "                epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "            ))\n",
        "    \n",
        "    # save the session\n",
        "    save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "    # init the test data array\n",
        "    test_acc_values = []\n",
        "    \n",
        "    # Check on the test data\n",
        "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, crop=crop, distort=False):\n",
        "        test_accuracy = sess.run(accuracy, feed_dict={\n",
        "            X: X_batch,\n",
        "            y: y_batch,\n",
        "            training: False\n",
        "        })\n",
        "        test_acc_values.append(test_accuracy)\n",
        "    \n",
        "    # average test accuracy across batches\n",
        "    test_acc = np.mean(test_acc_values)\n",
        "    \n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "# print results of last epoch\n",
        "print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "      epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model_7.13.4.7.7l ...\n",
            "Saving checkpoint\n",
            "Epoch 00 - step 391 - cv acc: 0.535 - train acc: 0.488 (mean) - cv cost: 1.270 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 01 - step 782 - cv acc: 0.709 - train acc: 0.652 (mean) - cv cost: 0.882 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 02 - step 1173 - cv acc: 0.712 - train acc: 0.712 (mean) - cv cost: 0.855 - lr: 0.00300\n",
            "Epoch 3 - cv acc: 0.7123 - train acc: 0.7121 (mean) - cv cost: 0.855\n",
            "Epoch 3 - cv acc: 0.7123 - train acc: 0.7121 (mean) - cv cost: 0.855\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kBB77PGV6Ovj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Scoring and Evaluating trained model"
      ]
    },
    {
      "metadata": {
        "id": "aVlCrRrXNDGF",
        "colab_type": "code",
        "outputId": "848965bb-62d3-4ede-a882-3351038ade6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Model : \", model_name)\n",
        "\n",
        "print(\"Convolutional network accuracy (test set):\",test_acc, \" Validation Set\", valid_acc_values[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model :  model_7.13.4.7.7l\n",
            "Convolutional network accuracy (test set): 0.7125  Validation Set 0.7123047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lAKoRfKIp6C5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "The CIFAR-10 dataset for image classification model using convolutional neural network gave an accuracy of 71% for 3 epoochs. This was the base model in which the parameters used were:\n",
        "* Activation Function: Rectified linear unit (ReLU)\n",
        "* Cost function: Cross-Entropy\n",
        "* No.of Epochs: 3\n",
        "* Gradient estimation: ADAM\n",
        "* Network Architecture:Number of layers: 10\n",
        "* Network initialization: zero\n",
        "\n",
        "I ran it for 3 epochs and got almost 71.25% accuracy. It can surely go much further since it was still undertrained! To improve the model the learning rate should be improved. Also, number of neurons can be more complicated for a better fit.Along, with that all, the parameters mentioned above can also be changed in order to improve the model accuracy.\n",
        "\n",
        "# References\n",
        "[1] https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
        "\n",
        "[2] http://www.cs.utoronto.ca/~kriz/cifar.html\n",
        "\n",
        "[3] https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c\n",
        "\n",
        "[4] https://www.kaggle.com/skooch/cifar-10-in-tensorflow/notebook\n"
      ]
    }
  ]
}
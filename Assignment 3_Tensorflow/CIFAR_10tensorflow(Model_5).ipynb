{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR_10tensorflow(Model 5).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "vC-dbqYD6wx5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ASSIGNMENT 3\n",
        "Using Tensorflow to build a CNN network for CIFAR-10 dataset. Each record is of size 1*3072. Building a CNN network to classify the data into the 10 classes.\n",
        "\n",
        "# Dataset\n",
        "CIFAR-10 dataset The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
        "\n",
        "The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n",
        "\n",
        "http://www.cs.utoronto.ca/~kriz/cifar.html"
      ]
    },
    {
      "metadata": {
        "id": "IgvNHJNB63tu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Installing pydrive"
      ]
    },
    {
      "metadata": {
        "id": "LV5zK2z1svHz",
        "colab_type": "code",
        "outputId": "3c8af8d7-a27e-4d6b-fd64-35746992966b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K    100% |████████████████████████████████| 993kB 17.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.11.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.4)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7iuwcNVp66XR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Creates connection"
      ]
    },
    {
      "metadata": {
        "id": "RdVJ01xcs22W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import tensorflow as tf\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFKXlTPf6_ps",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authenticating and creating the PyDrive client"
      ]
    },
    {
      "metadata": {
        "id": "uKyFMd7ns8B6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "upFdCwx67PQa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Getting ids of all the files in folder"
      ]
    },
    {
      "metadata": {
        "id": "2ND78D-1s_Ic",
        "colab_type": "code",
        "outputId": "728b623c-e250-45af-a56b-0e390387cc94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1DCFFw2O6BFq8Gk0eYu7JT4Qn224BNoCt' in parents and trashed=false\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: data_batch_1, id: 11Bo2ULl9_aOQ761ONc2vhepnydriELiT\n",
            "title: data_batch_2, id: 1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I\n",
            "title: test_batch, id: 1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp\n",
            "title: data_batch_3, id: 11ky6i6FSTGWJYOzXquELD4H-GUr49C4f\n",
            "title: data_batch_5, id: 1rmRytfjJWua0cv17DzST6PqoDFY2APa6\n",
            "title: data_batch_4, id: 1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U1xloFYh7Woz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "metadata": {
        "id": "jqFWHrJdtCsh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JaTAl_W77aQW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "Pha3ARZltJL0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D1bJwA3m7tqj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# if file is zipped"
      ]
    },
    {
      "metadata": {
        "id": "TXPeMz8itNux",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "zip_file = drive.CreateFile({'id': '11Bo2ULl9_aOQ761ONc2vhepnydriELiT'})\n",
        "zip_file.GetContentFile('data_batch_1')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1asFrGiOMdHKY-_KO94e1fLWMBN_Ke92I'})\n",
        "zip_file.GetContentFile('data_batch_2')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '11ky6i6FSTGWJYOzXquELD4H-GUr49C4f'})\n",
        "zip_file.GetContentFile('data_batch_3')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1bb6TRjqNY5A0FsD_P7s3ssepMGWNW-Eh'})\n",
        "zip_file.GetContentFile('data_batch_4')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1rmRytfjJWua0cv17DzST6PqoDFY2APa6'})\n",
        "zip_file.GetContentFile('data_batch_5')\n",
        "\n",
        "zip_file = drive.CreateFile({'id': '1Wyz_RdmoLe9r9t1rloap8AttSltmfwrp'})\n",
        "zip_file.GetContentFile('test_batch')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wn47go_-tSjj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data1 = unpickle(\"data_batch_1\")\n",
        "data2 = unpickle(\"data_batch_2\")\n",
        "data3 = unpickle(\"data_batch_3\")\n",
        "data4 = unpickle(\"data_batch_4\")\n",
        "data5 = unpickle(\"data_batch_5\")\n",
        "#label_data = unpickle('../input/batches.meta')[b'label_names']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4AVveFbtWS_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "labels1 = data1[b'labels']\n",
        "data1 = data1[b'data'] * 1.0\n",
        "labels2 = data2[b'labels']\n",
        "data2 = data2[b'data'] * 1.0\n",
        "labels3 = data3[b'labels']\n",
        "data3 = data3[b'data'] * 1.0\n",
        "labels4 = data4[b'labels']\n",
        "data4 = data4[b'data']  * 1.0\n",
        "labels5 = data5[b'labels']\n",
        "data5 = data5[b'data']  * 1.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "unZQ60fh7xz4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Combine the remaining four arrays to use as training data"
      ]
    },
    {
      "metadata": {
        "id": "RD4bDkq8taJv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_tr = np.concatenate([data1, data2, data3, data4, data5], axis=0)\n",
        "X_tr = np.dstack((X_tr[:, :1024], X_tr[:, 1024:2048], X_tr[:, 2048:])) / 1.0\n",
        "X_tr = (X_tr - 128) / 255.0\n",
        "X_tr = X_tr.reshape(-1, 32, 32, 3)\n",
        "\n",
        "y_tr = np.concatenate([labels1, labels2, labels3, labels4, labels5], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vkZ1V7H476kD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Setting the number of classes"
      ]
    },
    {
      "metadata": {
        "id": "X_BGbDZztdvV",
        "colab_type": "code",
        "outputId": "6e5edcac-5c91-451c-a4d5-9ef175bae116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "num_classes = len(np.unique(y_tr))\n",
        "\n",
        "print(\"X_tr\", X_tr.shape)\n",
        "print(\"y_tr\", y_tr.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_tr (50000, 32, 32, 3)\n",
            "y_tr (50000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cnkm-Xab8G-p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Importing the test data"
      ]
    },
    {
      "metadata": {
        "id": "LkwtLPYWtg89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data = unpickle(\"test_batch\")\n",
        "\n",
        "X_test = test_data[b'data']\n",
        "X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:])) / 1.0\n",
        "X_test = (X_test - 128) / 255.0\n",
        "X_test = X_test.reshape(-1, 32, 32, 3)\n",
        "y_test = np.asarray(test_data[b'labels'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s8AK6OkK8LxX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spliting into test and validation"
      ]
    },
    {
      "metadata": {
        "id": "xK0w1luWtmzW",
        "colab_type": "code",
        "outputId": "04275b77-d43f-4f26-d27e-ff4f728d0b47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X_te, X_cv, y_te, y_cv = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
        "\n",
        "print(\"X_te\", X_te.shape)\n",
        "print(\"X_cv\", X_cv.shape)\n",
        "print(\"y_te\", y_te.shape)\n",
        "print(\"y_cv\", y_cv.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_te (5000, 32, 32, 3)\n",
            "X_cv (5000, 32, 32, 3)\n",
            "y_te (5000,)\n",
            "y_cv (5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HRJASqst8Q6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch generator"
      ]
    },
    {
      "metadata": {
        "id": "m_bSW7PMtr8q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(X, y, batch_size, crop=False, distort=True):\n",
        "    # Shuffle X,y\n",
        "    shuffled_idx = np.arange(len(y))\n",
        "    np.random.shuffle(shuffled_idx)\n",
        "    i, h, w, c = X.shape\n",
        "    \n",
        "    # Enumerate indexes by steps of batch_size\n",
        "    for i in range(0, len(y), batch_size):\n",
        "        batch_idx = shuffled_idx[i:i+batch_size]\n",
        "        X_return = X[batch_idx]\n",
        "        \n",
        "        # optional random crop of images\n",
        "        if crop:\n",
        "            woff = (w - 24) // 4\n",
        "            hoff = (h - 24) // 4\n",
        "            startw = np.random.randint(low=woff,high=woff*2)\n",
        "            starth = np.random.randint(low=hoff,high=hoff*2)\n",
        "            X_return = X_return[:,startw:startw+24,starth:starth+24,:]\n",
        "       \n",
        "        # do random flipping of images\n",
        "        coin = np.random.binomial(1, 0.5, size=None)\n",
        "        if coin and distort:\n",
        "            X_return = X_return[...,::-1,:]\n",
        "        \n",
        "        yield X_return, y[batch_idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tGoQc41q8VMI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Configurations"
      ]
    },
    {
      "metadata": {
        "id": "9_f-SZuJttcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 250                   # how many epochs\n",
        "batch_size = 128\n",
        "steps_per_epoch = X_tr.shape[0] / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kK0TsaxR82P8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building the network\n",
        "\n",
        "\n",
        "## MODEL 7.13.4.6.7f\n",
        "\n",
        "Model description:\n",
        "\n",
        "* 7.6 - changed kernel reg rate to 0.01 from 0.1\n",
        "* 7.7 - optimize loss instead of ce 7.8 - remove redundant lambda, replaced scale in regularizer with lambda, changed lambda from 0.01 to 0.001\n",
        "* 7.9 - lambda 0 instead of 3\n",
        "* 7.9.1 - lambda 1 instead of 0\n",
        "* 7.9.2 - use lambda 2 instead of 1\n",
        "* 7.9.4f - use 3x3 pooling instead of 2x2\n",
        "* 7.11.6f - add batch norm after conv 5\n",
        "* 7.11.2f - raise lambda, add dropout after fc2\n",
        "* 7.12.2f - change fully connected dropout to 20%\n",
        "* 7.12.2.2g - change fc dropout to 25%, increase filters in last 2 conv layers to 192 from 128\n",
        "* 7.13.2.2f - change all pool sizes to 2x2 from 3x3\n",
        "* 7.13.3.6f - use different lambda for conv + fc layers\n"
      ]
    },
    {
      "metadata": {
        "id": "giFJrp_eueGh",
        "colab_type": "code",
        "outputId": "97f12315-3eee-4489-9174-4f5bf35970b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "cell_type": "code",
      "source": [
        "# Create new graph\n",
        "graph = tf.Graph()\n",
        "# whether to retrain model from scratch or use saved model\n",
        "init = True\n",
        "model_name = \"model_7.13.4.7.7l\"\n",
        "\n",
        "\n",
        "\n",
        "with graph.as_default():\n",
        "    # Placeholders\n",
        "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
        "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
        "    training = tf.placeholder(dtype=tf.bool)\n",
        "    \n",
        "    # create global step for decaying learning rate\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "\n",
        "    # lambda 6\n",
        "    lamC = 0.000050\n",
        "    lamF = 0.0025000\n",
        "           \n",
        "    # learning rate j\n",
        "    epochs_per_decay = 10\n",
        "    starting_rate = 0.003\n",
        "    decay_factor = 0.9\n",
        "    staircase = True\n",
        "    \n",
        "    learning_rate = tf.train.exponential_decay(starting_rate,                 # start at 0.003\n",
        "                                               global_step, \n",
        "                                               steps_per_epoch * epochs_per_decay,       # 100 epochs\n",
        "                                               decay_factor,                   # 0.5 decrease\n",
        "                                               staircase=staircase) \n",
        "    \n",
        "    # Small epsilon value for the BN transform\n",
        "    epsilon = 1e-3\n",
        "    \n",
        "    with tf.name_scope('conv1') as scope:\n",
        "        # Convolutional layer 1 \n",
        "        conv1 = tf.layers.conv2d(\n",
        "            X,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=10),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv1'                 \n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn1 = tf.layers.batch_normalization(\n",
        "            conv1,\n",
        "            axis=-1,\n",
        "            momentum=0.99,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn1'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv1_bn_relu = tf.nn.relu(bn1, name='relu1')\n",
        "\n",
        "        conv1_bn_relu = tf.layers.dropout(conv1_bn_relu, rate=0.1, seed=9, training=training)\n",
        "    \n",
        "    with tf.name_scope('conv2') as scope:\n",
        "        # Convolutional layer 2\n",
        "        conv2 = tf.layers.conv2d(\n",
        "            conv1_bn_relu,                           # Input data\n",
        "            filters=64,                  # 64 filters\n",
        "            kernel_size=(5, 5),          # Kernel size: 5x5\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=8),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv2'                  # Add name\n",
        "        )\n",
        "\n",
        "        # try batch normalization\n",
        "        bn2 = tf.layers.batch_normalization(\n",
        "            conv2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn2'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv2_bn_relu = tf.nn.relu(bn2, name='relu2')\n",
        "    \n",
        "    with tf.name_scope('pool1') as scope:\n",
        "         # Max pooling layer 1\n",
        "        pool1 = tf.layers.max_pooling2d(\n",
        "            conv2_bn_relu,                       # Input\n",
        "            pool_size=(2, 2),            # Pool size: 3x3\n",
        "            strides=(2, 2),              # Stride: 2\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            name='pool1'\n",
        "        )\n",
        "\n",
        "        # dropout at 10%\n",
        "        pool1 = tf.layers.dropout(pool1, rate=0.1, seed=1, training=training)\n",
        "\n",
        "    with tf.name_scope('conv3') as scope:\n",
        "        # Convolutional layer 3\n",
        "        conv3= tf.layers.conv2d(\n",
        "            pool1,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=7),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv3'                 \n",
        "        )\n",
        "\n",
        "        bn3 = tf.layers.batch_normalization(\n",
        "            conv3,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn3'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv3_bn_relu = tf.nn.relu(bn3, name='relu3')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        conv3_bn_relu = tf.layers.dropout(conv3_bn_relu, rate=0.1, seed=0, training=training)\n",
        "\n",
        "    with tf.name_scope('conv4') as scope:\n",
        "        # Convolutional layer 4\n",
        "        conv4= tf.layers.conv2d(\n",
        "            conv3_bn_relu,                       # Input\n",
        "            filters=96,                  # 96 filters\n",
        "            kernel_size=(4, 4),          # Kernel size: 4x4\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv4'                 \n",
        "        )\n",
        "\n",
        "        bn4 = tf.layers.batch_normalization(\n",
        "            conv4,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn4'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv4_bn_relu = tf.nn.relu(bn4, name='relu4')\n",
        "    \n",
        "    # Max pooling layer 2 \n",
        "    pool2 = tf.layers.max_pooling2d(\n",
        "        conv4_bn_relu,                       # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool2'\n",
        "    )\n",
        "\n",
        "    with tf.name_scope('conv5') as scope:\n",
        "        # Convolutional layer 5\n",
        "        conv5= tf.layers.conv2d(\n",
        "            pool2,                       # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,       \n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=2),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv5'                 \n",
        "        )\n",
        "        \n",
        "        \n",
        "        bn5 = tf.layers.batch_normalization(\n",
        "            conv5,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn5'\n",
        "        )\n",
        "        \n",
        "        # activation\n",
        "        conv5_bn_relu = tf.nn.relu(bn5, name='relu5')\n",
        "\n",
        "        # try dropout here\n",
        "        conv5_bn_relu = tf.layers.dropout(conv5_bn_relu, rate=0.1, seed=3, training=training)    \n",
        "\n",
        "    with tf.name_scope('conv6') as scope:\n",
        "        # Convolutional layer 6\n",
        "        conv6= tf.layers.conv2d(\n",
        "            conv5_bn_relu,               # Input\n",
        "            filters=128,                 # 128 filters\n",
        "            kernel_size=(3, 3),          # Kernel size: 3x3\n",
        "            strides=(1, 1),              # Stride: 1\n",
        "            padding='SAME',              # \"same\" padding\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=3), \n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
        "            name='conv6'                \n",
        "        )\n",
        "\n",
        "        bn6 = tf.layers.batch_normalization(\n",
        "            conv6,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn6'\n",
        "        )\n",
        "\n",
        "        #apply relu\n",
        "        conv6_bn_relu = tf.nn.relu(bn6, name='relu6')\n",
        "    \n",
        "    # Max pooling layer 3\n",
        "    pool3 = tf.layers.max_pooling2d(\n",
        "        conv6_bn_relu,               # input\n",
        "        pool_size=(2, 2),            # pool size 2x2\n",
        "        strides=(2, 2),              # stride 2\n",
        "        padding='SAME',\n",
        "        name='pool3'\n",
        "    )\n",
        "    \n",
        "    with tf.name_scope('flatten') as scope:\n",
        "        # Flatten output\n",
        "        flat_output = tf.contrib.layers.flatten(pool3)\n",
        "\n",
        "        # dropout at 10%\n",
        "        flat_output = tf.layers.dropout(flat_output, rate=0.1, seed=5, training=training)\n",
        "    \n",
        "    # Fully connected layer 1\n",
        "    with tf.name_scope('fc1') as scope:\n",
        "        fc1 = tf.layers.dense(\n",
        "            flat_output,                 # input\n",
        "            1024,                        # 1024 hidden units\n",
        "            activation=None,             # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=4),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc1\"\n",
        "        )\n",
        "        \n",
        "        bn7 = tf.layers.batch_normalization(\n",
        "            fc1,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn7'\n",
        "        )\n",
        "        \n",
        "        fc1_relu = tf.nn.relu(bn7, name='fc1_relu')\n",
        "        \n",
        "        # dropout at 25%\n",
        "        fc1_do = tf.layers.dropout(fc1_relu, rate=0.25, seed=10, training=training)\n",
        "    \n",
        "    # Fully connected layer 2\n",
        "    with tf.name_scope('fc2') as scope:\n",
        "        fc2 = tf.layers.dense(\n",
        "            fc1_do,                        # input\n",
        "            512,                        # 512 hidden units\n",
        "            activation=None,            # None\n",
        "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=5),\n",
        "            bias_initializer=tf.zeros_initializer(),\n",
        "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
        "            name=\"fc2\"\n",
        "        )\n",
        "        \n",
        "        bn8 = tf.layers.batch_normalization(\n",
        "            fc2,\n",
        "            axis=-1,\n",
        "            momentum=0.9,\n",
        "            epsilon=epsilon,\n",
        "            center=True,\n",
        "            scale=True,\n",
        "            beta_initializer=tf.zeros_initializer(),\n",
        "            gamma_initializer=tf.ones_initializer(),\n",
        "            moving_mean_initializer=tf.zeros_initializer(),\n",
        "            moving_variance_initializer=tf.ones_initializer(),\n",
        "            training=training,\n",
        "            name='bn8'\n",
        "        )\n",
        "        \n",
        "        fc2_relu = tf.nn.relu(bn8, name='fc2_relu')\n",
        "        \n",
        "        # dropout at 10%\n",
        "        fc2_do = tf.layers.dropout(fc2_relu, rate=0.25, seed=11, training=training)\n",
        "    \n",
        "    # Output layer\n",
        "    logits = tf.layers.dense(\n",
        "        fc2_do,                         # input\n",
        "        num_classes,                           # One output unit per category\n",
        "        activation=None,             # No activation function\n",
        "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=6),\n",
        "        bias_initializer=tf.zeros_initializer(),\n",
        "        name=\"logits\"\n",
        "    )\n",
        "    \n",
        "    # Kernel weights of the 1st conv. layer\n",
        "    with tf.variable_scope('conv1', reuse=True):\n",
        "        conv_kernels1 = tf.get_variable('kernel')\n",
        "    \n",
        "    # Mean cross-entropy\n",
        "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
        "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
        "    \n",
        "    # Adam optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    \n",
        "    # Minimize cross-entropy\n",
        "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
        "\n",
        "    # Compute predictions and accuracy\n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    is_correct = tf.equal(y, predictions)\n",
        "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
        "    \n",
        "    # add this so that the batch norm gets run\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "    # Create summary hooks\n",
        "    tf.summary.scalar('accuracy', accuracy)\n",
        "    tf.summary.scalar('cross_entropy', mean_ce)\n",
        "    tf.summary.scalar('learning_rate', learning_rate)\n",
        "    \n",
        "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
        "    merged = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-16-6638cb78506d>:59: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-6638cb78506d>:75: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.batch_normalization instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-6638cb78506d>:81: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dropout instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-16-6638cb78506d>:123: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-16-6638cb78506d>:298: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MUVFt9GZ9mYl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CONFIGURE OPTIONS"
      ]
    },
    {
      "metadata": {
        "id": "hAooQOtgujmS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "init = True                   # whether to initialize the model or use a saved version\n",
        "crop = False                  # do random cropping of images?\n",
        "\n",
        "meta_data_every = 5\n",
        "log_to_tensorboard = False\n",
        "print_every = 1                # how often to print metrics\n",
        "checkpoint_every = 1           # how often to save model in epochs\n",
        "use_gpu = True                 # whether or not to use the GPU\n",
        "print_metrics = True          # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
        "\n",
        "# Placeholders for metrics\n",
        "if init:\n",
        "    valid_acc_values = []\n",
        "    valid_cost_values = []\n",
        "    train_acc_values = []\n",
        "    train_cost_values = []\n",
        "    train_lr_values = []\n",
        "    train_loss_values = []\n",
        "    \n",
        "\n",
        "config = tf.ConfigProto()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5k43u7Pd9qUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Trainig the model"
      ]
    },
    {
      "metadata": {
        "id": "Jy-bZtiQurk5",
        "colab_type": "code",
        "outputId": "d540cb3b-b81b-48ab-d902-e773641c37d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8568
        }
      },
      "cell_type": "code",
      "source": [
        "with tf.Session(graph=graph, config=config) as sess:\n",
        "    if log_to_tensorboard:\n",
        "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
        "    \n",
        "    if not print_metrics:\n",
        "        # create a plot to be updated as model is trained\n",
        "        f, ax = plt.subplots(1,3,figsize=(20,5))\n",
        "    \n",
        "    # create the saver\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    # If the model is new initialize variables, else restore the session\n",
        "    if init:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "    else:\n",
        "        saver.restore(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "\n",
        "    # Set seed\n",
        "    np.random.seed(0)\n",
        "    \n",
        "    print(\"Training\", model_name, \"...\")\n",
        "    \n",
        "    # Train several epochs\n",
        "    for epoch in range(epochs):\n",
        "        # Accuracy values (train) after each batch\n",
        "        batch_acc = []\n",
        "        batch_cost = []\n",
        "        batch_loss = []\n",
        "        batch_lr = []\n",
        "        \n",
        "        # only log run metadata once per epoch\n",
        "        write_meta_data = False\n",
        "            \n",
        "        for X_batch, y_batch in get_batches(X_tr, y_tr, batch_size, crop=crop, distort=True):\n",
        "            if write_meta_data and log_to_tensboard:\n",
        "                # create the metadata\n",
        "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
        "                run_metadata = tf.RunMetadata()\n",
        "            \n",
        "                # Run training and evaluate accuracy\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                },\n",
        "                options=run_options,\n",
        "                run_metadata=run_metadata)\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                train_writer.add_run_metadata(run_metadata, 'step %d' % step)\n",
        "                train_writer.add_summary(summary, step)\n",
        "                write_meta_data = False\n",
        "                \n",
        "            else:\n",
        "                # Run training without meta data\n",
        "                _, _, summary, acc_value, cost_value, loss_value, step, lr = sess.run([train_op, extra_update_ops, merged, accuracy, mean_ce, loss, global_step, learning_rate], feed_dict={\n",
        "                    X: X_batch,\n",
        "                    y: y_batch,\n",
        "                    training: True\n",
        "                })\n",
        "\n",
        "                # Save accuracy (current batch)\n",
        "                batch_acc.append(acc_value)\n",
        "                batch_cost.append(cost_value)\n",
        "                batch_lr.append(lr)\n",
        "                batch_loss.append(loss_value)\n",
        "                \n",
        "                # write the summary\n",
        "                if log_to_tensorboard:\n",
        "                    train_writer.add_summary(summary, step)\n",
        "\n",
        "        # save checkpoint every nth epoch\n",
        "        if(epoch % checkpoint_every == 0):\n",
        "            print(\"Saving checkpoint\")\n",
        "            # save the model\n",
        "            save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "            # Now that model is saved set init to false so we reload it\n",
        "            init = False\n",
        "        \n",
        "        # init batch arrays\n",
        "        batch_cv_acc = []\n",
        "        batch_cv_cost = []\n",
        "        batch_cv_loss = []\n",
        "        \n",
        "        # Evaluate validation accuracy with batches so as to not crash the GPU\n",
        "        for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size, crop=crop, distort=False):\n",
        "            summary, valid_acc, valid_cost, valid_loss = sess.run([merged, accuracy, mean_ce, loss], feed_dict={\n",
        "                X: X_batch,\n",
        "                y: y_batch,\n",
        "                training: False\n",
        "            })\n",
        "\n",
        "            batch_cv_acc.append(valid_acc)\n",
        "            batch_cv_cost.append(valid_cost)\n",
        "            batch_cv_loss.append(valid_loss)\n",
        "\n",
        "        # Write average of validation data to summary logs\n",
        "        if log_to_tensorboard:\n",
        "            summary = tf.Summary(value=[tf.Summary.Value(tag=\"accuracy\", simple_value=np.mean(batch_cv_acc)),tf.Summary.Value(tag=\"cross_entropy\", simple_value=np.mean(batch_cv_cost)),])\n",
        "            test_writer.add_summary(summary, step)\n",
        "            step += 1\n",
        "            \n",
        "        # take the mean of the values to add to the metrics\n",
        "        valid_acc_values.append(np.mean(batch_cv_acc))\n",
        "        valid_cost_values.append(np.mean(batch_cv_cost))\n",
        "        train_acc_values.append(np.mean(batch_acc))\n",
        "        train_cost_values.append(np.mean(batch_cost))\n",
        "        train_lr_values.append(np.mean(batch_lr))\n",
        "        train_loss_values.append(np.mean(batch_loss))\n",
        "        \n",
        "        if print_metrics:\n",
        "            # Print progress every nth epoch to keep output to reasonable amount\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))\n",
        "        else:\n",
        "            # update the plot\n",
        "            ax[0].cla()\n",
        "            ax[0].plot(valid_acc_values, color=\"red\", label=\"Validation\")\n",
        "            ax[0].plot(train_acc_values, color=\"blue\", label=\"Training\")\n",
        "            ax[0].set_title('Validation accuracy: {:.4f} (mean last 3)'.format(np.mean(valid_acc_values[-3:])))\n",
        "            \n",
        "            # since we can't zoom in on plots like in tensorboard, scale y axis to give a decent amount of detail\n",
        "            if np.mean(valid_acc_values[-3:]) > 0.85:\n",
        "                ax[0].set_ylim([0.75,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.75:\n",
        "                ax[0].set_ylim([0.65,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.65:\n",
        "                ax[0].set_ylim([0.55,1.0])\n",
        "            elif np.mean(valid_acc_values[-3:]) > 0.55:\n",
        "                ax[0].set_ylim([0.45,1.0])           \n",
        "            \n",
        "            ax[0].set_xlabel('Epoch')\n",
        "            ax[0].set_ylabel('Accuracy')\n",
        "            ax[0].legend()\n",
        "            \n",
        "            ax[1].cla()\n",
        "            ax[1].plot(valid_cost_values, color=\"red\", label=\"Validation\")\n",
        "            ax[1].plot(train_cost_values, color=\"blue\", label=\"Training\")\n",
        "            ax[1].set_title('Validation xentropy: {:.3f} (mean last 3)'.format(np.mean(valid_cost_values[-3:])))\n",
        "            ax[1].set_xlabel('Epoch')\n",
        "            ax[1].set_ylabel('Cross Entropy')\n",
        "            ax[1].legend()\n",
        "            \n",
        "            ax[2].cla()\n",
        "            ax[2].plot(train_lr_values)\n",
        "            ax[2].set_title(\"Learning rate: {:.6f}\".format(np.mean(train_lr_values[-1:])))\n",
        "            ax[2].set_xlabel(\"Epoch\")\n",
        "            ax[2].set_ylabel(\"Learning Rate\")\n",
        "            \n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "            \n",
        "        # Print data every 50th epoch so I can write it down to compare models\n",
        "        if (not print_metrics) and (epoch % 50 == 0) and (epoch > 1):\n",
        "            if(epoch % print_every == 0):\n",
        "                print('Epoch {:02d} - step {} - cv acc: {:.3f} - train acc: {:.3f} (mean) - cv cost: {:.3f} - lr: {:.5f}'.format(\n",
        "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost), lr\n",
        "                ))  \n",
        "            \n",
        "    # print results of last epoch\n",
        "    print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "                epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "            ))\n",
        "    \n",
        "    # save the session\n",
        "    save_path = saver.save(sess, './model/cifar_'+model_name+'.ckpt')\n",
        "    \n",
        "    # init the test data array\n",
        "    test_acc_values = []\n",
        "    \n",
        "    # Check on the test data\n",
        "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, crop=crop, distort=False):\n",
        "        test_accuracy = sess.run(accuracy, feed_dict={\n",
        "            X: X_batch,\n",
        "            y: y_batch,\n",
        "            training: False\n",
        "        })\n",
        "        test_acc_values.append(test_accuracy)\n",
        "    \n",
        "    # average test accuracy across batches\n",
        "    test_acc = np.mean(test_acc_values)\n",
        "    \n",
        "# show the plot\n",
        "plt.show()\n",
        "\n",
        "# print results of last epoch\n",
        "print('Epoch {} - cv acc: {:.4f} - train acc: {:.4f} (mean) - cv cost: {:.3f}'.format(\n",
        "      epochs, np.mean(batch_cv_acc), np.mean(batch_acc), np.mean(batch_cv_cost)\n",
        "    ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model_7.13.4.7.7l ...\n",
            "Saving checkpoint\n",
            "Epoch 00 - step 391 - cv acc: 0.530 - train acc: 0.487 (mean) - cv cost: 1.336 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 01 - step 782 - cv acc: 0.695 - train acc: 0.649 (mean) - cv cost: 0.898 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 02 - step 1173 - cv acc: 0.730 - train acc: 0.714 (mean) - cv cost: 0.783 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 03 - step 1564 - cv acc: 0.722 - train acc: 0.745 (mean) - cv cost: 0.830 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 04 - step 1955 - cv acc: 0.775 - train acc: 0.764 (mean) - cv cost: 0.663 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 05 - step 2346 - cv acc: 0.768 - train acc: 0.784 (mean) - cv cost: 0.702 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 06 - step 2737 - cv acc: 0.776 - train acc: 0.798 (mean) - cv cost: 0.715 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 07 - step 3128 - cv acc: 0.730 - train acc: 0.812 (mean) - cv cost: 0.842 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 08 - step 3519 - cv acc: 0.787 - train acc: 0.820 (mean) - cv cost: 0.650 - lr: 0.00300\n",
            "Saving checkpoint\n",
            "Epoch 09 - step 3910 - cv acc: 0.806 - train acc: 0.828 (mean) - cv cost: 0.606 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 10 - step 4301 - cv acc: 0.820 - train acc: 0.840 (mean) - cv cost: 0.544 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 11 - step 4692 - cv acc: 0.822 - train acc: 0.848 (mean) - cv cost: 0.559 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 12 - step 5083 - cv acc: 0.842 - train acc: 0.854 (mean) - cv cost: 0.491 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 13 - step 5474 - cv acc: 0.820 - train acc: 0.859 (mean) - cv cost: 0.574 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 14 - step 5865 - cv acc: 0.826 - train acc: 0.858 (mean) - cv cost: 0.529 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 15 - step 6256 - cv acc: 0.851 - train acc: 0.866 (mean) - cv cost: 0.463 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 16 - step 6647 - cv acc: 0.846 - train acc: 0.868 (mean) - cv cost: 0.489 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 17 - step 7038 - cv acc: 0.840 - train acc: 0.873 (mean) - cv cost: 0.522 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 18 - step 7429 - cv acc: 0.839 - train acc: 0.877 (mean) - cv cost: 0.496 - lr: 0.00270\n",
            "Saving checkpoint\n",
            "Epoch 19 - step 7820 - cv acc: 0.859 - train acc: 0.878 (mean) - cv cost: 0.436 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 20 - step 8211 - cv acc: 0.846 - train acc: 0.887 (mean) - cv cost: 0.474 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 21 - step 8602 - cv acc: 0.851 - train acc: 0.891 (mean) - cv cost: 0.468 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 22 - step 8993 - cv acc: 0.857 - train acc: 0.893 (mean) - cv cost: 0.432 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 23 - step 9384 - cv acc: 0.864 - train acc: 0.894 (mean) - cv cost: 0.435 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 24 - step 9775 - cv acc: 0.874 - train acc: 0.895 (mean) - cv cost: 0.408 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 25 - step 10166 - cv acc: 0.872 - train acc: 0.894 (mean) - cv cost: 0.417 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 26 - step 10557 - cv acc: 0.847 - train acc: 0.895 (mean) - cv cost: 0.489 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 27 - step 10948 - cv acc: 0.870 - train acc: 0.898 (mean) - cv cost: 0.410 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 28 - step 11339 - cv acc: 0.860 - train acc: 0.901 (mean) - cv cost: 0.422 - lr: 0.00243\n",
            "Saving checkpoint\n",
            "Epoch 29 - step 11730 - cv acc: 0.876 - train acc: 0.901 (mean) - cv cost: 0.403 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 30 - step 12121 - cv acc: 0.839 - train acc: 0.907 (mean) - cv cost: 0.497 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 31 - step 12512 - cv acc: 0.858 - train acc: 0.910 (mean) - cv cost: 0.449 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 32 - step 12903 - cv acc: 0.875 - train acc: 0.909 (mean) - cv cost: 0.387 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 33 - step 13294 - cv acc: 0.872 - train acc: 0.911 (mean) - cv cost: 0.410 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 34 - step 13685 - cv acc: 0.877 - train acc: 0.913 (mean) - cv cost: 0.411 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 35 - step 14076 - cv acc: 0.861 - train acc: 0.914 (mean) - cv cost: 0.436 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 36 - step 14467 - cv acc: 0.880 - train acc: 0.914 (mean) - cv cost: 0.386 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 37 - step 14858 - cv acc: 0.869 - train acc: 0.914 (mean) - cv cost: 0.427 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 38 - step 15249 - cv acc: 0.866 - train acc: 0.916 (mean) - cv cost: 0.447 - lr: 0.00219\n",
            "Saving checkpoint\n",
            "Epoch 39 - step 15640 - cv acc: 0.882 - train acc: 0.915 (mean) - cv cost: 0.375 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 40 - step 16031 - cv acc: 0.881 - train acc: 0.921 (mean) - cv cost: 0.376 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 41 - step 16422 - cv acc: 0.875 - train acc: 0.923 (mean) - cv cost: 0.398 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 42 - step 16813 - cv acc: 0.874 - train acc: 0.926 (mean) - cv cost: 0.422 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 43 - step 17204 - cv acc: 0.879 - train acc: 0.924 (mean) - cv cost: 0.392 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 44 - step 17595 - cv acc: 0.885 - train acc: 0.924 (mean) - cv cost: 0.388 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 45 - step 17986 - cv acc: 0.862 - train acc: 0.925 (mean) - cv cost: 0.430 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 46 - step 18377 - cv acc: 0.885 - train acc: 0.927 (mean) - cv cost: 0.380 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 47 - step 18768 - cv acc: 0.879 - train acc: 0.927 (mean) - cv cost: 0.386 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 48 - step 19159 - cv acc: 0.879 - train acc: 0.926 (mean) - cv cost: 0.375 - lr: 0.00197\n",
            "Saving checkpoint\n",
            "Epoch 49 - step 19550 - cv acc: 0.881 - train acc: 0.928 (mean) - cv cost: 0.377 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 50 - step 19941 - cv acc: 0.883 - train acc: 0.933 (mean) - cv cost: 0.406 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 51 - step 20332 - cv acc: 0.890 - train acc: 0.934 (mean) - cv cost: 0.365 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 52 - step 20723 - cv acc: 0.893 - train acc: 0.935 (mean) - cv cost: 0.363 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 53 - step 21114 - cv acc: 0.880 - train acc: 0.935 (mean) - cv cost: 0.413 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 54 - step 21505 - cv acc: 0.887 - train acc: 0.934 (mean) - cv cost: 0.368 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 55 - step 21896 - cv acc: 0.887 - train acc: 0.934 (mean) - cv cost: 0.381 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 56 - step 22287 - cv acc: 0.884 - train acc: 0.935 (mean) - cv cost: 0.367 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 57 - step 22678 - cv acc: 0.884 - train acc: 0.936 (mean) - cv cost: 0.382 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 58 - step 23069 - cv acc: 0.884 - train acc: 0.937 (mean) - cv cost: 0.380 - lr: 0.00177\n",
            "Saving checkpoint\n",
            "Epoch 59 - step 23460 - cv acc: 0.890 - train acc: 0.937 (mean) - cv cost: 0.365 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 60 - step 23851 - cv acc: 0.895 - train acc: 0.940 (mean) - cv cost: 0.347 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 61 - step 24242 - cv acc: 0.891 - train acc: 0.943 (mean) - cv cost: 0.364 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 62 - step 24633 - cv acc: 0.892 - train acc: 0.942 (mean) - cv cost: 0.362 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 63 - step 25024 - cv acc: 0.890 - train acc: 0.942 (mean) - cv cost: 0.382 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 64 - step 25415 - cv acc: 0.872 - train acc: 0.945 (mean) - cv cost: 0.435 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 65 - step 25806 - cv acc: 0.885 - train acc: 0.943 (mean) - cv cost: 0.389 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 66 - step 26197 - cv acc: 0.880 - train acc: 0.945 (mean) - cv cost: 0.423 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 67 - step 26588 - cv acc: 0.882 - train acc: 0.945 (mean) - cv cost: 0.415 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 68 - step 26979 - cv acc: 0.887 - train acc: 0.944 (mean) - cv cost: 0.385 - lr: 0.00159\n",
            "Saving checkpoint\n",
            "Epoch 69 - step 27370 - cv acc: 0.885 - train acc: 0.944 (mean) - cv cost: 0.386 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 70 - step 27761 - cv acc: 0.896 - train acc: 0.948 (mean) - cv cost: 0.370 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 71 - step 28152 - cv acc: 0.883 - train acc: 0.951 (mean) - cv cost: 0.413 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 72 - step 28543 - cv acc: 0.893 - train acc: 0.951 (mean) - cv cost: 0.367 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 73 - step 28934 - cv acc: 0.896 - train acc: 0.949 (mean) - cv cost: 0.363 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 74 - step 29325 - cv acc: 0.892 - train acc: 0.950 (mean) - cv cost: 0.374 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 75 - step 29716 - cv acc: 0.885 - train acc: 0.950 (mean) - cv cost: 0.413 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 76 - step 30107 - cv acc: 0.887 - train acc: 0.951 (mean) - cv cost: 0.387 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 77 - step 30498 - cv acc: 0.880 - train acc: 0.953 (mean) - cv cost: 0.433 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 78 - step 30889 - cv acc: 0.893 - train acc: 0.950 (mean) - cv cost: 0.370 - lr: 0.00143\n",
            "Saving checkpoint\n",
            "Epoch 79 - step 31280 - cv acc: 0.887 - train acc: 0.951 (mean) - cv cost: 0.443 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 80 - step 31671 - cv acc: 0.881 - train acc: 0.954 (mean) - cv cost: 0.404 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 81 - step 32062 - cv acc: 0.884 - train acc: 0.958 (mean) - cv cost: 0.412 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 82 - step 32453 - cv acc: 0.889 - train acc: 0.956 (mean) - cv cost: 0.384 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 83 - step 32844 - cv acc: 0.896 - train acc: 0.957 (mean) - cv cost: 0.354 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 84 - step 33235 - cv acc: 0.880 - train acc: 0.956 (mean) - cv cost: 0.413 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 85 - step 33626 - cv acc: 0.889 - train acc: 0.958 (mean) - cv cost: 0.378 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 86 - step 34017 - cv acc: 0.891 - train acc: 0.956 (mean) - cv cost: 0.388 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 87 - step 34408 - cv acc: 0.902 - train acc: 0.956 (mean) - cv cost: 0.370 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 88 - step 34799 - cv acc: 0.889 - train acc: 0.956 (mean) - cv cost: 0.387 - lr: 0.00129\n",
            "Saving checkpoint\n",
            "Epoch 89 - step 35190 - cv acc: 0.891 - train acc: 0.957 (mean) - cv cost: 0.392 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 90 - step 35581 - cv acc: 0.895 - train acc: 0.962 (mean) - cv cost: 0.388 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 91 - step 35972 - cv acc: 0.894 - train acc: 0.960 (mean) - cv cost: 0.381 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 92 - step 36363 - cv acc: 0.890 - train acc: 0.961 (mean) - cv cost: 0.387 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 93 - step 36754 - cv acc: 0.880 - train acc: 0.961 (mean) - cv cost: 0.459 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 94 - step 37145 - cv acc: 0.894 - train acc: 0.961 (mean) - cv cost: 0.388 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 95 - step 37536 - cv acc: 0.893 - train acc: 0.961 (mean) - cv cost: 0.406 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 96 - step 37927 - cv acc: 0.896 - train acc: 0.962 (mean) - cv cost: 0.384 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 97 - step 38318 - cv acc: 0.893 - train acc: 0.961 (mean) - cv cost: 0.392 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 98 - step 38709 - cv acc: 0.892 - train acc: 0.963 (mean) - cv cost: 0.423 - lr: 0.00116\n",
            "Saving checkpoint\n",
            "Epoch 99 - step 39100 - cv acc: 0.892 - train acc: 0.962 (mean) - cv cost: 0.399 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 100 - step 39491 - cv acc: 0.897 - train acc: 0.964 (mean) - cv cost: 0.387 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 101 - step 39882 - cv acc: 0.891 - train acc: 0.965 (mean) - cv cost: 0.391 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 102 - step 40273 - cv acc: 0.899 - train acc: 0.966 (mean) - cv cost: 0.385 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 103 - step 40664 - cv acc: 0.895 - train acc: 0.966 (mean) - cv cost: 0.396 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 104 - step 41055 - cv acc: 0.892 - train acc: 0.966 (mean) - cv cost: 0.397 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 105 - step 41446 - cv acc: 0.893 - train acc: 0.967 (mean) - cv cost: 0.411 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 106 - step 41837 - cv acc: 0.893 - train acc: 0.967 (mean) - cv cost: 0.403 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 107 - step 42228 - cv acc: 0.887 - train acc: 0.966 (mean) - cv cost: 0.390 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 108 - step 42619 - cv acc: 0.887 - train acc: 0.965 (mean) - cv cost: 0.429 - lr: 0.00105\n",
            "Saving checkpoint\n",
            "Epoch 109 - step 43010 - cv acc: 0.897 - train acc: 0.966 (mean) - cv cost: 0.393 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 110 - step 43401 - cv acc: 0.907 - train acc: 0.969 (mean) - cv cost: 0.371 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 111 - step 43792 - cv acc: 0.900 - train acc: 0.969 (mean) - cv cost: 0.403 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 112 - step 44183 - cv acc: 0.899 - train acc: 0.970 (mean) - cv cost: 0.377 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 113 - step 44574 - cv acc: 0.896 - train acc: 0.969 (mean) - cv cost: 0.375 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 114 - step 44965 - cv acc: 0.886 - train acc: 0.970 (mean) - cv cost: 0.438 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 115 - step 45356 - cv acc: 0.900 - train acc: 0.971 (mean) - cv cost: 0.381 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 116 - step 45747 - cv acc: 0.902 - train acc: 0.970 (mean) - cv cost: 0.361 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 117 - step 46138 - cv acc: 0.898 - train acc: 0.968 (mean) - cv cost: 0.407 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 118 - step 46529 - cv acc: 0.902 - train acc: 0.970 (mean) - cv cost: 0.370 - lr: 0.00094\n",
            "Saving checkpoint\n",
            "Epoch 119 - step 46920 - cv acc: 0.897 - train acc: 0.970 (mean) - cv cost: 0.392 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 120 - step 47311 - cv acc: 0.899 - train acc: 0.972 (mean) - cv cost: 0.415 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 121 - step 47702 - cv acc: 0.901 - train acc: 0.973 (mean) - cv cost: 0.411 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 122 - step 48093 - cv acc: 0.895 - train acc: 0.973 (mean) - cv cost: 0.388 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 123 - step 48484 - cv acc: 0.901 - train acc: 0.972 (mean) - cv cost: 0.391 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 124 - step 48875 - cv acc: 0.889 - train acc: 0.973 (mean) - cv cost: 0.433 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 125 - step 49266 - cv acc: 0.905 - train acc: 0.973 (mean) - cv cost: 0.391 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 126 - step 49657 - cv acc: 0.903 - train acc: 0.974 (mean) - cv cost: 0.391 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 127 - step 50048 - cv acc: 0.897 - train acc: 0.972 (mean) - cv cost: 0.380 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 128 - step 50439 - cv acc: 0.903 - train acc: 0.975 (mean) - cv cost: 0.391 - lr: 0.00085\n",
            "Saving checkpoint\n",
            "Epoch 129 - step 50830 - cv acc: 0.906 - train acc: 0.973 (mean) - cv cost: 0.385 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 130 - step 51221 - cv acc: 0.900 - train acc: 0.976 (mean) - cv cost: 0.395 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 131 - step 51612 - cv acc: 0.900 - train acc: 0.976 (mean) - cv cost: 0.418 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 132 - step 52003 - cv acc: 0.899 - train acc: 0.975 (mean) - cv cost: 0.410 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 133 - step 52394 - cv acc: 0.896 - train acc: 0.976 (mean) - cv cost: 0.407 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 134 - step 52785 - cv acc: 0.897 - train acc: 0.975 (mean) - cv cost: 0.423 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 135 - step 53176 - cv acc: 0.896 - train acc: 0.976 (mean) - cv cost: 0.407 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 136 - step 53567 - cv acc: 0.896 - train acc: 0.976 (mean) - cv cost: 0.420 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 137 - step 53958 - cv acc: 0.897 - train acc: 0.977 (mean) - cv cost: 0.423 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 138 - step 54349 - cv acc: 0.905 - train acc: 0.975 (mean) - cv cost: 0.381 - lr: 0.00076\n",
            "Saving checkpoint\n",
            "Epoch 139 - step 54740 - cv acc: 0.902 - train acc: 0.976 (mean) - cv cost: 0.402 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 140 - step 55131 - cv acc: 0.898 - train acc: 0.978 (mean) - cv cost: 0.450 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 141 - step 55522 - cv acc: 0.891 - train acc: 0.978 (mean) - cv cost: 0.461 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 142 - step 55913 - cv acc: 0.900 - train acc: 0.980 (mean) - cv cost: 0.436 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 143 - step 56304 - cv acc: 0.899 - train acc: 0.979 (mean) - cv cost: 0.419 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 144 - step 56695 - cv acc: 0.901 - train acc: 0.978 (mean) - cv cost: 0.383 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 145 - step 57086 - cv acc: 0.902 - train acc: 0.979 (mean) - cv cost: 0.414 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 146 - step 57477 - cv acc: 0.904 - train acc: 0.978 (mean) - cv cost: 0.404 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 147 - step 57868 - cv acc: 0.902 - train acc: 0.979 (mean) - cv cost: 0.426 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 148 - step 58259 - cv acc: 0.886 - train acc: 0.977 (mean) - cv cost: 0.460 - lr: 0.00069\n",
            "Saving checkpoint\n",
            "Epoch 149 - step 58650 - cv acc: 0.894 - train acc: 0.980 (mean) - cv cost: 0.445 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 150 - step 59041 - cv acc: 0.897 - train acc: 0.981 (mean) - cv cost: 0.445 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 151 - step 59432 - cv acc: 0.902 - train acc: 0.981 (mean) - cv cost: 0.406 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 152 - step 59823 - cv acc: 0.896 - train acc: 0.981 (mean) - cv cost: 0.420 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 153 - step 60214 - cv acc: 0.899 - train acc: 0.981 (mean) - cv cost: 0.451 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 154 - step 60605 - cv acc: 0.903 - train acc: 0.981 (mean) - cv cost: 0.421 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 155 - step 60996 - cv acc: 0.900 - train acc: 0.979 (mean) - cv cost: 0.411 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 156 - step 61387 - cv acc: 0.894 - train acc: 0.981 (mean) - cv cost: 0.436 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 157 - step 61778 - cv acc: 0.898 - train acc: 0.980 (mean) - cv cost: 0.417 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 158 - step 62169 - cv acc: 0.903 - train acc: 0.981 (mean) - cv cost: 0.420 - lr: 0.00062\n",
            "Saving checkpoint\n",
            "Epoch 159 - step 62560 - cv acc: 0.900 - train acc: 0.981 (mean) - cv cost: 0.453 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 160 - step 62951 - cv acc: 0.901 - train acc: 0.982 (mean) - cv cost: 0.441 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 161 - step 63342 - cv acc: 0.901 - train acc: 0.983 (mean) - cv cost: 0.426 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 162 - step 63733 - cv acc: 0.904 - train acc: 0.982 (mean) - cv cost: 0.420 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 163 - step 64124 - cv acc: 0.905 - train acc: 0.982 (mean) - cv cost: 0.441 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 164 - step 64515 - cv acc: 0.900 - train acc: 0.983 (mean) - cv cost: 0.423 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 165 - step 64906 - cv acc: 0.901 - train acc: 0.983 (mean) - cv cost: 0.426 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 166 - step 65297 - cv acc: 0.902 - train acc: 0.983 (mean) - cv cost: 0.418 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 167 - step 65688 - cv acc: 0.902 - train acc: 0.983 (mean) - cv cost: 0.463 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 168 - step 66079 - cv acc: 0.902 - train acc: 0.983 (mean) - cv cost: 0.407 - lr: 0.00056\n",
            "Saving checkpoint\n",
            "Epoch 169 - step 66470 - cv acc: 0.902 - train acc: 0.983 (mean) - cv cost: 0.438 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 170 - step 66861 - cv acc: 0.902 - train acc: 0.984 (mean) - cv cost: 0.422 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 171 - step 67252 - cv acc: 0.900 - train acc: 0.985 (mean) - cv cost: 0.414 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 172 - step 67643 - cv acc: 0.905 - train acc: 0.984 (mean) - cv cost: 0.419 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 173 - step 68034 - cv acc: 0.892 - train acc: 0.985 (mean) - cv cost: 0.481 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 174 - step 68425 - cv acc: 0.903 - train acc: 0.985 (mean) - cv cost: 0.423 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 175 - step 68816 - cv acc: 0.904 - train acc: 0.985 (mean) - cv cost: 0.437 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 176 - step 69207 - cv acc: 0.895 - train acc: 0.984 (mean) - cv cost: 0.456 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 177 - step 69598 - cv acc: 0.901 - train acc: 0.985 (mean) - cv cost: 0.442 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 178 - step 69989 - cv acc: 0.903 - train acc: 0.984 (mean) - cv cost: 0.430 - lr: 0.00050\n",
            "Saving checkpoint\n",
            "Epoch 179 - step 70380 - cv acc: 0.905 - train acc: 0.984 (mean) - cv cost: 0.439 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 180 - step 70771 - cv acc: 0.901 - train acc: 0.986 (mean) - cv cost: 0.431 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 181 - step 71162 - cv acc: 0.893 - train acc: 0.986 (mean) - cv cost: 0.479 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 182 - step 71553 - cv acc: 0.895 - train acc: 0.986 (mean) - cv cost: 0.483 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 183 - step 71944 - cv acc: 0.903 - train acc: 0.986 (mean) - cv cost: 0.432 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 184 - step 72335 - cv acc: 0.894 - train acc: 0.986 (mean) - cv cost: 0.468 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 185 - step 72726 - cv acc: 0.899 - train acc: 0.987 (mean) - cv cost: 0.453 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 186 - step 73117 - cv acc: 0.901 - train acc: 0.986 (mean) - cv cost: 0.458 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 187 - step 73508 - cv acc: 0.899 - train acc: 0.986 (mean) - cv cost: 0.441 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 188 - step 73899 - cv acc: 0.898 - train acc: 0.986 (mean) - cv cost: 0.442 - lr: 0.00045\n",
            "Saving checkpoint\n",
            "Epoch 189 - step 74290 - cv acc: 0.901 - train acc: 0.987 (mean) - cv cost: 0.460 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 190 - step 74681 - cv acc: 0.898 - train acc: 0.987 (mean) - cv cost: 0.457 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 191 - step 75072 - cv acc: 0.903 - train acc: 0.988 (mean) - cv cost: 0.428 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 192 - step 75463 - cv acc: 0.904 - train acc: 0.988 (mean) - cv cost: 0.449 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 193 - step 75854 - cv acc: 0.900 - train acc: 0.988 (mean) - cv cost: 0.460 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 194 - step 76245 - cv acc: 0.901 - train acc: 0.988 (mean) - cv cost: 0.456 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 195 - step 76636 - cv acc: 0.898 - train acc: 0.988 (mean) - cv cost: 0.468 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 196 - step 77027 - cv acc: 0.893 - train acc: 0.988 (mean) - cv cost: 0.497 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 197 - step 77418 - cv acc: 0.899 - train acc: 0.988 (mean) - cv cost: 0.457 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 198 - step 77809 - cv acc: 0.896 - train acc: 0.988 (mean) - cv cost: 0.484 - lr: 0.00041\n",
            "Saving checkpoint\n",
            "Epoch 199 - step 78200 - cv acc: 0.900 - train acc: 0.988 (mean) - cv cost: 0.465 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 200 - step 78591 - cv acc: 0.899 - train acc: 0.988 (mean) - cv cost: 0.477 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 201 - step 78982 - cv acc: 0.903 - train acc: 0.989 (mean) - cv cost: 0.443 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 202 - step 79373 - cv acc: 0.903 - train acc: 0.989 (mean) - cv cost: 0.462 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 203 - step 79764 - cv acc: 0.903 - train acc: 0.988 (mean) - cv cost: 0.467 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 204 - step 80155 - cv acc: 0.897 - train acc: 0.990 (mean) - cv cost: 0.489 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 205 - step 80546 - cv acc: 0.906 - train acc: 0.989 (mean) - cv cost: 0.467 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 206 - step 80937 - cv acc: 0.898 - train acc: 0.989 (mean) - cv cost: 0.464 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 207 - step 81328 - cv acc: 0.905 - train acc: 0.989 (mean) - cv cost: 0.433 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 208 - step 81719 - cv acc: 0.906 - train acc: 0.990 (mean) - cv cost: 0.472 - lr: 0.00036\n",
            "Saving checkpoint\n",
            "Epoch 209 - step 82110 - cv acc: 0.908 - train acc: 0.990 (mean) - cv cost: 0.436 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 210 - step 82501 - cv acc: 0.897 - train acc: 0.990 (mean) - cv cost: 0.473 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 211 - step 82892 - cv acc: 0.904 - train acc: 0.990 (mean) - cv cost: 0.449 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 212 - step 83283 - cv acc: 0.904 - train acc: 0.990 (mean) - cv cost: 0.471 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 213 - step 83674 - cv acc: 0.902 - train acc: 0.990 (mean) - cv cost: 0.469 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 214 - step 84065 - cv acc: 0.904 - train acc: 0.989 (mean) - cv cost: 0.440 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 215 - step 84456 - cv acc: 0.901 - train acc: 0.990 (mean) - cv cost: 0.454 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 216 - step 84847 - cv acc: 0.907 - train acc: 0.990 (mean) - cv cost: 0.445 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 217 - step 85238 - cv acc: 0.908 - train acc: 0.990 (mean) - cv cost: 0.448 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 218 - step 85629 - cv acc: 0.904 - train acc: 0.990 (mean) - cv cost: 0.468 - lr: 0.00033\n",
            "Saving checkpoint\n",
            "Epoch 219 - step 86020 - cv acc: 0.907 - train acc: 0.990 (mean) - cv cost: 0.458 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 220 - step 86411 - cv acc: 0.905 - train acc: 0.990 (mean) - cv cost: 0.454 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 221 - step 86802 - cv acc: 0.906 - train acc: 0.991 (mean) - cv cost: 0.444 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 222 - step 87193 - cv acc: 0.908 - train acc: 0.992 (mean) - cv cost: 0.472 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 223 - step 87584 - cv acc: 0.904 - train acc: 0.992 (mean) - cv cost: 0.456 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 224 - step 87975 - cv acc: 0.905 - train acc: 0.990 (mean) - cv cost: 0.468 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 225 - step 88366 - cv acc: 0.908 - train acc: 0.992 (mean) - cv cost: 0.472 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 226 - step 88757 - cv acc: 0.898 - train acc: 0.991 (mean) - cv cost: 0.498 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 227 - step 89148 - cv acc: 0.909 - train acc: 0.990 (mean) - cv cost: 0.464 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 228 - step 89539 - cv acc: 0.901 - train acc: 0.991 (mean) - cv cost: 0.486 - lr: 0.00030\n",
            "Saving checkpoint\n",
            "Epoch 229 - step 89930 - cv acc: 0.901 - train acc: 0.991 (mean) - cv cost: 0.455 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 230 - step 90321 - cv acc: 0.903 - train acc: 0.992 (mean) - cv cost: 0.453 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 231 - step 90712 - cv acc: 0.902 - train acc: 0.992 (mean) - cv cost: 0.487 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 232 - step 91103 - cv acc: 0.906 - train acc: 0.991 (mean) - cv cost: 0.481 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 233 - step 91494 - cv acc: 0.906 - train acc: 0.992 (mean) - cv cost: 0.500 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 234 - step 91885 - cv acc: 0.902 - train acc: 0.991 (mean) - cv cost: 0.481 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 235 - step 92276 - cv acc: 0.898 - train acc: 0.992 (mean) - cv cost: 0.501 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 236 - step 92667 - cv acc: 0.905 - train acc: 0.992 (mean) - cv cost: 0.489 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 237 - step 93058 - cv acc: 0.904 - train acc: 0.991 (mean) - cv cost: 0.485 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 238 - step 93449 - cv acc: 0.908 - train acc: 0.992 (mean) - cv cost: 0.481 - lr: 0.00027\n",
            "Saving checkpoint\n",
            "Epoch 239 - step 93840 - cv acc: 0.906 - train acc: 0.992 (mean) - cv cost: 0.481 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 240 - step 94231 - cv acc: 0.912 - train acc: 0.992 (mean) - cv cost: 0.462 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 241 - step 94622 - cv acc: 0.905 - train acc: 0.992 (mean) - cv cost: 0.464 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 242 - step 95013 - cv acc: 0.907 - train acc: 0.994 (mean) - cv cost: 0.460 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 243 - step 95404 - cv acc: 0.908 - train acc: 0.992 (mean) - cv cost: 0.489 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 244 - step 95795 - cv acc: 0.903 - train acc: 0.993 (mean) - cv cost: 0.514 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 245 - step 96186 - cv acc: 0.905 - train acc: 0.993 (mean) - cv cost: 0.497 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 246 - step 96577 - cv acc: 0.906 - train acc: 0.992 (mean) - cv cost: 0.485 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 247 - step 96968 - cv acc: 0.905 - train acc: 0.992 (mean) - cv cost: 0.485 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 248 - step 97359 - cv acc: 0.905 - train acc: 0.993 (mean) - cv cost: 0.492 - lr: 0.00024\n",
            "Saving checkpoint\n",
            "Epoch 249 - step 97750 - cv acc: 0.899 - train acc: 0.992 (mean) - cv cost: 0.495 - lr: 0.00022\n",
            "Epoch 250 - cv acc: 0.8986 - train acc: 0.9923 (mean) - cv cost: 0.495\n",
            "Epoch 250 - cv acc: 0.8986 - train acc: 0.9923 (mean) - cv cost: 0.495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PKUjQtjW9zkg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Scoring and Evaluating trained model"
      ]
    },
    {
      "metadata": {
        "id": "Z8ajpNPUuwhh",
        "colab_type": "code",
        "outputId": "416e9a97-291f-4e99-e6b9-7214c117a4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## MODEL 7.20.0.11g - no crop, 250 epochs\n",
        "print(\"Model : \", model_name)\n",
        "\n",
        "print(\"Convolutional network accuracy (test set):\",test_acc, \" Validation Set\", valid_acc_values[-1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model :  model_7.13.4.7.7l\n",
            "Convolutional network accuracy (test set): 0.9060547  Validation Set 0.8986328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-MaiylE7-Ayx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "The CIFAR-10 dataset for image classification model using convolutional neural network gave an accuracy of 90.% for 250 epoochs. This was the model in which the parameters used were:\n",
        "\n",
        "* Activation Function: Rectified linear unit (ReLU)\n",
        "* Cost function: Cross-Entropy\n",
        "* No.of Epochs: 250\n",
        "* Gradient estimation: ADAM\n",
        "* Network Architecture:Number of layers: 12\n",
        "* Network initialization: zero\n",
        "\n",
        "I ran it for 250 epochs and got 90% accuracy. This is my best model after changing all the parameters.\n",
        "\n",
        "# References\n",
        "[1] https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
        "\n",
        "[2] http://www.cs.utoronto.ca/~kriz/cifar.html\n",
        "\n",
        "[3] https://towardsdatascience.com/cifar-10-image-classification-in-tensorflow-5b501f7dc77c\n",
        "\n",
        "[4] https://www.kaggle.com/skooch/cifar-10-in-tensorflow/notebook"
      ]
    },
    {
      "metadata": {
        "id": "xC6INe1pHBBW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# License\n",
        "Must have a license such as the MIT License https://opensource.org/licenses/MIT MIT License\n",
        "\n",
        "Copyright (c) 2019 Rajeshree Kale\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ]
}